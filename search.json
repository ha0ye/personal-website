[{"authors":null,"categories":["R"],"content":"  Introduction Setup The easy case (all eigenvalues are real) The hard case (complex eigenvalues) Demonstration Conclusions References   Introduction Lately, I’ve been stuck in getting an intuition for exactly what is going on when a real matrix has complex eigenvalues (and complex eigenvectors) accordingly. After consulting various sources, and playing around with some examples, I think I have a grasp on what’s going on, and translating the math into an interpretation in the original space.\n Setup Suppose we have a real, square matrix \\(\\mathbf{A}\\) of dimensions \\(n \\times n\\). One standard interpretation of \\(\\mathbf{A}\\) is that it operates on vectors \\(\\mathbf{x} \\in \\mathbb{R}^n\\) via multiplication: \\(\\mathbf{A} \\mathbf{x}\\) is the result of “applying” \\(\\mathbf{A}\\) to \\(\\mathbf{x}\\).\nIf \\(\\mathbf{A}\\) is diagonalizable, then \\(\\mathbf{A}\\) has \\(n\\) eigenvalues and \\(n\\) eigenvectors.\n The easy case (all eigenvalues are real) When the eigenvalues are all real numbers, there is a straightforward interpretation that results from the definition of eigenvalues and eigenvectors. For an eigenvalue, \\(\\lambda\\) and corresponding eigenvector \\(\\mathbf{v}\\): \\[\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}\\].\nSo the transformation that results from multiplying by \\(\\mathbf{A}\\) can be viewed as stretching, by a factor of \\(\\lambda\\), along the \\(n\\) axes defined by the eigenvectors, \\(\\mathbf{v}\\). Thus, the result of \\(\\mathbf{A} \\mathbf{x}\\) is equivalent to: decomposing \\(\\mathbf{x}\\) into the basis (defined by the eigenvectors) and stretching along each of the basis vectors (according to the eigenvalues).\nNote: The eigenvectors are not necessarily orthogonal, but will nevertheless still span the space of \\(\\mathbf{x}\\) as a group.\nSo far, so good, right? But even when \\(\\mathbf{A}\\) contains only real numbers, it can still have complex eigenvalues and eigenvectors.\nThis makes interpretation a bit more difficult: if \\(\\mathbf{x}\\) is some actual thing (e.g. species abundances) and \\(\\mathbf{A}\\) is some actual transformation, what does it mean for the component of \\(\\mathbf{x}\\) along \\(\\mathbf{v}\\) to be stretched by a factor of \\(\\lambda\\); if \\(\\mathbf{v}\\) and \\(\\lambda\\) are complex numbers?\n The hard case (complex eigenvalues) Nearly every resource I could find about interpreting complex eigenvalues and eigenvectors mentioned that in addition to a stretching, the transformation imposed by \\(\\mathbf{A}\\) involved rotation. So ideally, we should be able to identify the axis of rotation and the angle of rotation from the eigenvalue and eigenvector. (Here’s where I got a bit stuck with resources, since all the detailed notes involved 2D matrices, and there is no need to specify an axis of rotation.)\nFirst, note that the complex eigenvalues and eigenvectors have to occur in complex-conjugate pairs; because \\(\\mathbf{A}\\) is all reals. So, for one such pair of eigenvalues, \\(\\lambda_1\\) and \\(\\lambda_2\\), \\(\\lambda_1 = \\overline{\\lambda_2}\\), and for the corresponding eigenvectors, \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\), \\(\\mathbf{v}_1 = \\overline{\\mathbf{v}_2}\\).\nIn other words, we have to consider the pair of eigenvalues and eigenvectors together. Luckily, the interpretation makes things work out well so that it’s still just one rotation, just split in two halves:\n the angle of rotation is given by \\(\\mathrm{Arg}(\\lambda)\\), so \\(\\lambda_1\\) and \\(\\lambda_2\\) are the same rotation angle, just in opposite directions the plane of rotation is defined by the vectors \\(\\mathrm{Re}(\\mathbf{v})\\) and \\(\\mathrm{Im}(\\mathbf{v})\\), so \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\) define the same plane, but with normal vectors in opposite directions thust the resulting operation defined by \\(\\lambda_1\\) and \\(\\mathbf{v}_1\\) is the same as that defined by \\(\\lambda_2\\) and \\(\\mathbf{v}_2\\)  To figure out what components of \\(\\mathbf{x}\\) undergo this rotation, we turn to the eigendecomposition definition: \\[\\mathbf{A} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^{-1}\\]\nwhere \\(\\mathbf{Q}\\) is the matrix with the eigenvectors as columns and \\(\\mathbf{\\Lambda}\\) is the diagonal matrix whose values are the eigenvalues.\nThus, the component of \\(\\mathbf{x}\\) that undergoes the rotation specified by \\(\\lambda_1\\) and \\(\\mathbf{v}_1\\) is given by multiplying row \\(1\\) of \\(\\mathbf{Q}^{-1}\\) by \\(\\mathbf{x}\\) (which gives us the 2d coordinates of \\(\\mathbf{x}\\) in the plane of rotation, specified as a complex number).\n Demonstration Ok, let’s demonstrate this with a 4-dimensional matrix, starting with its eigendecomposition.\nA \u0026lt;- matrix(c(3, -2, 0, 1, 4, -1, 1, -1, 2, 1, 0, 2, 1, 0, -2, 0), byrow = TRUE, nrow = 4) Q \u0026lt;- eigen(A)$vectors L \u0026lt;- eigen(A)$values Q_inv \u0026lt;- solve(Q) Next, we’ll choose a random vector and compute its coordinates in the 2D rotational subspace: (I’m a bit unsure about the minus sign on the imaginary component, but maybe I got mixed up about whether the rotation should be clockwise or counter-clockwise.)\n# choose a random vector set.seed(42) x \u0026lt;- matrix(rnorm(4), nrow = 4) # coordinates in Re(v1), Im(v1) basis space pp1 \u0026lt;- Q_inv[1, ] %*% x XX1 \u0026lt;- matrix(c(Re(pp1), -Im(pp1)), nrow = 2) Define the transformations that will occur: the rotation and scaling, and the projection back into the full 4D space:\n# back-transformation matrix VV1 \u0026lt;- matrix(c(Re(Q[, 1]), Im(Q[, 1])), ncol = 2) # rotation by Arg(l1), scaling by Mod(l1) LL1 \u0026lt;- matrix(c(Re(L[1]), Im(L[1]), -Im(L[1]), Re(L[1])), nrow = 2, byrow = TRUE) # check this definition theta \u0026lt;- Arg(L[1]) all.equal(LL1, Mod(L[1]) * matrix(c(cos(theta), sin(theta), -sin(theta), cos(theta)), byrow = TRUE, nrow = 2)) ## [1] TRUE Now compute the result of this transformation of this component of \\(x\\):\nzz1 \u0026lt;- VV1 %*% LL1 %*% XX1 Now, because we also have a 2nd pair of complex eigenvalues and eigenvectors, repeat with the 3rd eigenvalue:\n# coordinates in basis Re(v3), Im(v3) pp3 \u0026lt;- Q_inv[3, ] %*% x XX3 \u0026lt;- matrix(c(Re(pp3), -Im(pp3)), nrow = 2) # back-transformation matrix VV3 \u0026lt;- matrix(c(Re(Q[, 3]), Im(Q[, 3])), ncol = 2) # rotation by Arg(l3), scaling by Mod(l3) LL3 \u0026lt;- matrix(c(Re(L[3]), Im(L[3]), -Im(L[3]), Re(L[3])), nrow = 2, byrow = TRUE) zz3 \u0026lt;- VV3 %*% LL3 %*% XX3 We can check whether these outputs are the equivalent to applying \\(\\mathbf{A}\\) to the components of \\(\\mathbf{x}\\):\n# decompose x into xx1, xx2, xx3, xx4 xx \u0026lt;- Q_inv %*% x xx1 \u0026lt;- t(xx[1, 1] %*% Q[, 1]) xx2 \u0026lt;- t(xx[2, 1] %*% Q[, 2]) xx3 \u0026lt;- t(xx[3, 1] %*% Q[, 3]) xx4 \u0026lt;- t(xx[4, 1] %*% Q[, 4]) # check that xx1, xx2, xx3, xx4 add up to x all.equal(Re(xx1 + xx2 + xx3 + xx4), x) ## [1] TRUE # check that A (xx1 + xx2) is what we computed previously, 2 * zz1 all.equal(2 * zz1, Re(A %*% (xx1 + xx2))) ## [1] TRUE # check that A (xx3 + xx4) is what we computed previously, 2 * zz3 all.equal(2 * zz3, Re(A %*% (xx3 + xx4))) ## [1] TRUE # check that A x = 2 * zz1 + 2 * zz3 all.equal(2 * zz1 + 2 * zz3, A %*% x) ## [1] TRUE  Conclusions So, armed with this knowledge, I can now describe the transformation that corresponds to a specific complex-conjugate pair of eigenvalues and eigenvectors. Alas, specifying the angle and axis of rotation is quite a bit harder than saying which relative proportions in a real eigenvector are getting scaled, but the above demonstration provides tools for decomposing \\(\\mathbf{x}\\) into the components that are getting transformed, as well as the result. (both of which exist as vectors in the original state space of \\(\\mathbb{R}^n\\))\n References Various notes that I consulted:\nhttp://www.math.utk.edu/~freire/complex-eig2005.pdf https://math.stackexchange.com/questions/1546104/complex-eigenvalues-of-a-rotation-matrix   ","date":1575504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"60a6f9dab2e9b81a4f19bcb452278f2b","permalink":"https://ha0ye.github.io/personal-website/post/2019-12-05-interpreting-complex-eigenvalues/","publishdate":"2019-12-05T00:00:00Z","relpermalink":"/personal-website/post/2019-12-05-interpreting-complex-eigenvalues/","section":"post","summary":"Introduction Setup The easy case (all eigenvalues are real) The hard case (complex eigenvalues) Demonstration Conclusions References   Introduction Lately, I’ve been stuck in getting an intuition for exactly what is going on when a real matrix has complex eigenvalues (and complex eigenvectors) accordingly. After consulting various sources, and playing around with some examples, I think I have a grasp on what’s going on, and translating the math into an interpretation in the original space.","tags":["linear algebra"],"title":"Interpreting Complex Eigenvalues of Real Matrices","type":"post"},{"authors":null,"categories":["R"],"content":"  Introduction My Use Case Workflow Building the Docker image Uploading the docker image to Docker Hub Setting up Travis to use the Docker image  References   Introduction The below summarize the workflow I’ve converged on, after reading through various tutorials on Docker, examples, etc.\nIf you’re here, I presume you have some interest in R package development and/or using Docker, which is a tool for containerizing an environment for running software.\nSo why another blogpost about it? Well, a lot of the information I found seems to focus on specific use cases of Docker, that is NOT what I want to do. Sometimes there is additional fluff that is unexplained and/or unnecessary for my use case.\n My Use Case Here’s what I want to do:\n develop an R package, assume it’s past minimum prototype stage, and so dependencies are mostly fixed create a Docker image capturing everything needed to build/run/test the R package set up Continuous Integration to use the Docker image for automated testing  The last is especially important for some of my projects, because the set of dependencies can be large enough, that installing all of them takes a bit of time. Although Travis CI can cache dependencies, it has to fully complete its script without errors in order to cache, but will also time out with an error at 50 minutes.\nIn the past, I used incremental builds – only include some of the dependencies, bypass tests and checks, to get Travis to cache some subset of the dependencies. Then, repeat with more and more, until they’re all cached. This is fine, up until something changes (like an R version), and the process has to be repeated.\nHence, the goal of moving dependencies into a Docker image.\n Workflow Building the Docker image First, we need to create a docker image that has everything needed to build and test the R package. The Rocker Project has many pre-build images for R, which can be seen at https://www.rocker-project.org/images/. I used rocker/verse as a base, because both devtools and the “publishing-related packages” are relevant for my package development needs.\nWithin my R package, I add a Dockerfile file that contains instructions for building a custom image on top of rocker/verse:\n# base image: rocker/verse (with a specific version of R) # has R, RStudio, tidyverse, devtools, tex, and publishing-related packages FROM rocker/verse:3.6.0 # required MAINTAINER Hao Ye \u0026lt;lhopitalified@gmail.com\u0026gt; # copy the repo contents into the docker image at `/portalDS` COPY . /portalDS # install the dependencies of the R package located at `/portalDS` RUN apt-get -y update -qq \\ \u0026amp;\u0026amp; apt-get install -y --no-install-recommends \\ libgsl0-dev \\ \u0026amp;\u0026amp; R -e \u0026quot;devtools::install_dev_deps(\u0026#39;/portalDS\u0026#39;, dep = TRUE)\u0026quot; \\ \u0026amp;\u0026amp; R -e \u0026quot;install.packages(\u0026#39;tidyr\u0026#39;, repos=\u0026#39;http://cran.rstudio.com/\u0026#39;)\u0026quot; \\ \u0026amp;\u0026amp; R -e \u0026quot;install.packages(\u0026#39;testthat\u0026#39;, repos=\u0026#39;http://cran.rstudio.com/\u0026#39;)\u0026quot; Note that we had to include instructions for an additional system dependency required for one of the R package dependencies. It can be tedious to figure this out, because of what things are named, and how long it takes to build the Docker image, so details on testing this out are going to be in a later post.\nAn additional note: I noticed my unit tests depended on new versions of tidyr and testthat that, for some reason, weren’t getting included in the docker image. I ended up using manual install commands, and specifying the RStudio CRAN mirror to get this to work.\nIn the command line, I then use:\ndocker build -t haoye/portal_ds . to build the docker image.\n Uploading the docker image to Docker Hub Because we want Travis CI to grab our pre-built Docker image, we need to put the image someplace. Luckily, Docker Hub is set up to accomodate this. Once we have setup our account and configured our machine to be able to communicate with docker hub, I can use:\ndocker push haoye/portal_ds to send the built image to docker hub.\n Setting up Travis to use the Docker image Our Travis script is then going to include instructiors to retrieve the Docker image, update the R package files inside it, and do whatever tests and checks we would typically do in Travis.\nNote a few extra items. We use a env_copy.sh script to copy over environmental variables from Travis into the running docker container. Specifically, we want the variables that are used to detect that tests are being run on Travis, and the authentication to push a code coverage report.\nFinally, at the end, we copy the generated pkgdown docs back to Travis, and use the deploy: scripting to build a new website for the docs, if everything checked out on master branch.\nenv: global: - REPO=haoye/portal_ds sudo: required warnings_are_errors: false language: generic services: - docker before_install: # copy environmental variables to a file - bash env_copy.sh # retrieve the docker container from docker hub - docker pull $REPO # run the docker container and copy over the files into the container - docker run --env-file env_file --name portalds -t -d $REPO /bin/bash - docker exec -i portalds bash -c \u0026quot;rm -fr /portalDS\u0026quot; - rm env_file - docker cp ../portalDS portalds:/ script: # navigate into the directory and run devtools::check on the package - docker exec -i portalds bash -c \u0026quot;cd portalDS \u0026amp;\u0026amp; Rscript -e \u0026#39;devtools::check()\u0026#39;\u0026quot; after_success: # run code coverage, pkgdown, deploy new pkgdown docs - docker exec -i portalds bash -c \u0026quot;R CMD INSTALL portalDS\u0026quot; - docker exec -i portalds bash -c \u0026quot;cd portalDS \u0026amp;\u0026amp; Rscript -e \u0026#39;covr::codecov()\u0026#39;\u0026quot; - docker exec -i portalds bash -c \u0026quot;cd portalDS \u0026amp;\u0026amp; Rscript -e \u0026#39;pkgdown::build_site()\u0026#39;\u0026quot; - docker cp portalds:/portalDS/docs docs deploy: provider: pages skip-cleanup: true github-token: $GITHUB_PAT keep-history: true local-dir: docs on: branch: master   References Various places I consulted for similar instructions, help with Docker, etc.\nhttps://alexandereckert.com/post/testing-r-packages-with-latest-r-devel/ https://github.com/grunwaldlab/poppr/wiki/R-CMD-check-with-docker-rocker https://lecardozo.github.io/2018/03/01/automated-docker-build.html this twitter thread   ","date":1570665600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"bcd3a279ac157761fc3f11adc7f1fd11","permalink":"https://ha0ye.github.io/personal-website/post/2019-10-10-docker-for-r-package-development/","publishdate":"2019-10-10T00:00:00Z","relpermalink":"/personal-website/post/2019-10-10-docker-for-r-package-development/","section":"post","summary":"Introduction My Use Case Workflow Building the Docker image Uploading the docker image to Docker Hub Setting up Travis to use the Docker image  References   Introduction The below summarize the workflow I’ve converged on, after reading through various tutorials on Docker, examples, etc.\nIf you’re here, I presume you have some interest in R package development and/or using Docker, which is a tool for containerizing an environment for running software.","tags":["Docker","Travis CI","package"],"title":"Docker Setup for R package Development","type":"post"},{"authors":null,"categories":null,"content":"","date":1569024000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"c23fce9c7e6b7c5475e648de3e2a58d3","permalink":"https://ha0ye.github.io/personal-website/project/gpedm/","publishdate":"2019-09-21T00:00:00Z","relpermalink":"/personal-website/project/gpedm/","section":"project","summary":"Gaussian Process Regression for Empirical Dynamic Modeling","tags":["Software","R","Time Series","Dynamics"],"title":"GPEDM","type":"project"},{"authors":null,"categories":["R"],"content":"  Overview Setup Initial data examination Injuries by date Injuries by cause   Overview This is an exploration of the TidyTuesday dataset on “Amusement Park Injuries”, done as part of the Wednesday coding clinic for Research Bazaar Gainesville.\n Setup ## load packages library(tidyverse) ## ── Attaching packages ─────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.2.1 ✔ purrr 0.3.2 ## ✔ tibble 2.1.3 ✔ dplyr 0.8.3 ## ✔ tidyr 1.0.0 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0 ## ── Conflicts ────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(lubridate) ## ## Attaching package: \u0026#39;lubridate\u0026#39; ## The following object is masked from \u0026#39;package:base\u0026#39;: ## ## date ## get the raw data tx_injuries \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-09-10/tx_injuries.csv\u0026quot;) ## Parsed with column specification: ## cols( ## injury_report_rec = col_double(), ## name_of_operation = col_character(), ## city = col_character(), ## st = col_character(), ## injury_date = col_character(), ## ride_name = col_character(), ## serial_no = col_character(), ## gender = col_character(), ## age = col_character(), ## body_part = col_character(), ## alleged_injury = col_character(), ## cause_of_injury = col_character(), ## other = col_character() ## ) safer_parks \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-09-10/saferparks.csv\u0026quot;) ## Parsed with column specification: ## cols( ## .default = col_character(), ## acc_id = col_double(), ## num_injured = col_double(), ## age_youngest = col_double(), ## mechanical = col_double(), ## op_error = col_double(), ## employee = col_double() ## ) ## See spec(...) for full column specifications.  Initial data examination We compute some basic summaries on the data, to see what analyses might be supported. For instance, if the record keeping does discretization on the body part injured, that would facilitate analyses that look into relative injuries to different body parts.\nlength(unique(tx_injuries$body_part)) ## [1] 189 head(tx_injuries$body_part, n = 20) ## [1] \u0026quot;Mouth\u0026quot; \u0026quot;Knee\u0026quot; ## [3] \u0026quot;Right Shoulder\u0026quot; \u0026quot;Lower Leg\u0026quot; ## [5] \u0026quot;Head\u0026quot; \u0026quot;Bottom of foot\u0026quot; ## [7] \u0026quot;Right shoulder, right knee\u0026quot; \u0026quot;Back\u0026quot; ## [9] \u0026quot;Neck and back\u0026quot; \u0026quot;Left leg\u0026quot; ## [11] \u0026quot;Shoulder\u0026quot; \u0026quot;Back \u0026amp; leg\u0026quot; ## [13] \u0026quot;Tail bone\u0026quot; \u0026quot;Ankle\u0026quot; ## [15] \u0026quot;Foot \u0026amp; leg\u0026quot; \u0026quot;Shoulder\u0026quot; ## [17] \u0026quot;Ankle\u0026quot; \u0026quot;Tooth\u0026quot; ## [19] \u0026quot;Neck\u0026quot; \u0026quot;Lower back\u0026quot; There is some discretization, but depending on whether multiple body parts are injured, or whether the record indicated details (such as [right or left] “ankle” injury), it would be a hassle to try and convert this data column into a usable categorical variable.\nlength(unique(tx_injuries$ride_name)) ## [1] 252 head(tx_injuries$ride_name, n = 20) ## [1] \u0026quot;I Fly\u0026quot; \u0026quot;Gulf Glider\u0026quot; ## [3] \u0026quot;Howlin Tornado\u0026quot; \u0026quot;Scooby Doo Ghost Blasters\u0026quot; ## [5] \u0026quot;Alien Abduction\u0026quot; \u0026quot;Go Karts\u0026quot; ## [7] \u0026quot;Gold River Adventure\u0026quot; \u0026quot;Titan\u0026quot; ## [9] \u0026quot;Wild River\u0026quot; \u0026quot;Sky Ride 2\u0026quot; ## [11] \u0026quot;iFly Austin\u0026quot; \u0026quot;Go Karts\u0026quot; ## [13] \u0026quot;LaVibra\u0026quot; \u0026quot;Go Kart Track\u0026quot; ## [15] \u0026quot;Rock 2 Drop 2 Rockwall\u0026quot; \u0026quot;I Fly\u0026quot; ## [17] \u0026quot;Zipline # 6\u0026quot; \u0026quot;Batman\u0026quot; ## [19] \u0026quot;New York Adventure\u0026quot; \u0026quot;Texas Tumble\u0026quot; The ride_name column seems to be abotu the same, with some record having generic names for rides, and some having a unique commerical name.\n Injuries by date First, let’s select just a few columns of interest, rename them to be more convenient for us, and then plot injuries by date.\n# wrangle the first dataset of amusement park injuries df_1 \u0026lt;- tx_injuries %\u0026gt;% select(park = name_of_operation, city, state = st, ride = ride_name, body_part, injury_type = alleged_injury, date = injury_date) %\u0026gt;% mutate(date_mdy = mdy(date), date_serial = as.Date(as.numeric(date), origin = \u0026quot;1899-12-30\u0026quot;), date = if_else(is.na(date_mdy), date_serial, date_mdy)) ## Warning: 349 failed to parse. ## Warning in as.Date(as.numeric(date), origin = \u0026quot;1899-12-30\u0026quot;): NAs introduced ## by coercion # sum up injuries by date df_1 %\u0026gt;% count(date) %\u0026gt;% {.} -\u0026gt; to_plot # plot injuries by date to_plot %\u0026gt;% ggplot(aes(x = date, y = n)) + geom_line() + theme_bw() + labs(y = \u0026quot;Number of Injuries\u0026quot;) + guides(color = \u0026quot;none\u0026quot;) ## Warning: Removed 1 rows containing missing values (geom_path). Besides the expected peak during the summer, we can also see hints of an earlier bump in injuries earlier in the year, perhaps corresponding to spring break.\n Injuries by cause Next, we wanted to look at potential sources of injuries:\nlength(unique(tx_injuries$cause_of_injury)) ## [1] 368 head(tx_injuries$cause_of_injury) ## [1] \u0026quot;Student attempted unfamiliar manuever\u0026quot; ## [2] \u0026quot;Hit her knee on a chair on the ride\u0026quot; ## [3] \u0026quot;Injured person fell out of raft and hit her shoulder on the slide\u0026quot; ## [4] \u0026quot;Guest backed into advancing car\u0026quot; ## [5] \u0026quot;unknown guest did not report to operator or attendant\u0026quot; ## [6] \u0026quot;had taken her shoes off and was driving barefoot - hit railing\u0026quot; The description of injury causes in the first dataset would require lots of work, so let’s move on.\nThe safer_parks dataset has columns for whether the source of the injury/report was mechanical failure, operator error, employee error, or NA for all three. (I think operator error and employee error are distinct, because of cases like go karts, where the operator is a customer as opposed to an employee of the park in charge of running the ride, as for a roller-coaster.)\nFirst, let’s verify that there are no data points in more than one category:\nsafer_parks %\u0026gt;% filter((!is.na(mechanical) + !is.na(op_error) + !is.na(employee)) \u0026gt; 1) %\u0026gt;% dim() ## [1] 0 23 No rows here means that in each data point, at most one of the values in those 3 columns is not an NA.\nNow let’s organize the data and see how the rates of causes differ, sorted by descending number of total accidents per state.\n# there\u0026#39;s almost certainly a better way than nested ifelse statements, but this # example only has a few and is faster than trying to look up the right way of # re-coding the data safer_parks %\u0026gt;% mutate(state = acc_state, cause = ifelse(!is.na(mechanical), \u0026quot;mechanical\u0026quot;, ifelse(!is.na(op_error), \u0026quot;operator error\u0026quot;, ifelse(!is.na(employee), \u0026quot;employee error\u0026quot;, \u0026quot;other\u0026quot;)))) %\u0026gt;% {.} -\u0026gt; df_2 df_2 %\u0026gt;% count(state, cause) %\u0026gt;% spread(cause, n, fill = 0) %\u0026gt;% mutate(total = `employee error` + mechanical + `operator error` + other) %\u0026gt;% arrange(desc(total)) %\u0026gt;% knitr::kable()   state employee error mechanical operator error other total    CA 1 131 25 2840 2997  PA 9 31 20 1823 1883  FL 0 32 6 936 974  TX 0 5 3 543 551  NJ 3 10 1 506 520  OK 3 31 18 352 404  NH 0 13 2 196 211  KY 0 4 1 113 118  NC 1 12 7 78 98  MI 0 8 5 84 97  IL 0 16 6 68 90  AZ 0 4 0 84 88  WI 0 7 6 47 60  OH 0 4 8 35 47  IA 3 8 2 19 32  ME 1 4 4 23 32  MD 0 4 1 20 25  TN 1 4 0 18 23  MA 0 5 2 8 15  WA 0 9 0 6 15  CO 0 4 3 6 13  CT 0 5 0 6 11  WV 0 4 4 3 11  AR 0 1 3 4 8  MO 0 3 0 2 5  NY 0 1 0 3 4  SC 0 2 1 0 3  MN 0 1 0 1 2  MT 0 1 0 1 2  NM 0 2 0 0 2  AK 0 1 0 0 1  GA 0 1 0 0 1  ID 0 0 1 0 1  KS 0 1 0 0 1  NE 0 1 0 0 1  NV 0 0 0 1 1  OR 0 1 0 0 1  UT 0 1 0 0 1  VA 0 1 0 0 1  WY 0 1 0 0 1    Hmm, it’s curious why PA is so high up on the list, given that it isn’t either a really large state or one known for many parks. Some possible hypotheses:\n different reporting means PA has more reports different types of parks means PA parks have higher injury rates different parks (maybe one or a few parks in PA are particularly injury-prone) different date range (the dataset may capture a total period of time different for PA relative to other states)   ","date":1568160000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"985e7a38b47441a4d7e2e7b75142f133","permalink":"https://ha0ye.github.io/personal-website/post/2019-09-11-amusement-park-injuries/","publishdate":"2019-09-11T00:00:00Z","relpermalink":"/personal-website/post/2019-09-11-amusement-park-injuries/","section":"post","summary":"Overview Setup Initial data examination Injuries by date Injuries by cause   Overview This is an exploration of the TidyTuesday dataset on “Amusement Park Injuries”, done as part of the Wednesday coding clinic for Research Bazaar Gainesville.\n Setup ## load packages library(tidyverse) ## ── Attaching packages ─────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.2.1 ✔ purrr 0.3.2 ## ✔ tibble 2.1.3 ✔ dplyr 0.8.3 ## ✔ tidyr 1.","tags":["TidyTuesday","ResBazGNV"],"title":"Amusement Park Injuries #TidyTuesday","type":"post"},{"authors":null,"categories":null,"content":"","date":1551225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"6152d9fca622a3ca80433e20eca4c296","permalink":"https://ha0ye.github.io/personal-website/project/matss/","publishdate":"2019-02-27T00:00:00Z","relpermalink":"/personal-website/project/matss/","section":"project","summary":"R package for Macroecological Analysis of Time Series Structure","tags":["Software","Data","R","Time Series"],"title":"MATSS","type":"project"},{"authors":null,"categories":null,"content":"","date":1551225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"df7a931179118a9e5e43ec4d0dfa8049","permalink":"https://ha0ye.github.io/personal-website/project/matss-forecasting/","publishdate":"2019-02-27T00:00:00Z","relpermalink":"/personal-website/project/matss-forecasting/","section":"project","summary":"Analyzing Ecological Predictability using MATSS","tags":["Software","Data","R","Time Series"],"title":"MATSS-forecasting","type":"project"},{"authors":["Erica M. Christensen","Glenda M. Yenni","Hao Ye","Juniper L. Simonis","Ellen K. Bledsoe","Renata Diaz","Shawn D. Taylor","Ethan P. White","S. K. Morgan Ernest"],"categories":null,"content":"","date":1548288000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"95a3ab607989fa399f5b676573ead327","permalink":"https://ha0ye.github.io/personal-website/publication/2019_portalr/","publishdate":"2019-01-24T00:00:00Z","relpermalink":"/personal-website/publication/2019_portalr/","section":"publication","summary":"Data sharing, the practice of making data widely available for use by others, benefits the ecological research community in ensuring study reproducibility and enabling many data sets to be combined to address broad-scale questions (e.g. meta-analyses). However, there are few standards or rules about how ecological data are collected and stored, and so it can be difficult and/or time-consuming to understand how to use unfamiliar data appropriately. `portalr` is an R package (R Core Team, 2018) to facilitate working with the data collected from a long-term ecological study called The Portal Project. All data is freely available in a GitHub repository (https://github.com/weecology/PortalData) and archived on Zenodo (Ernest et al., 2018a). The purpose of `portalr` is to allow new users to quickly and easily gain access to the data and start to use it, and to allow experienced users to produce analyses more efficiently and consistently. The functions contained in this package were developed by the researchers most familiar with the data and are designed to reflect best use practices.","tags":["Data","Software","R package"],"title":"portalr: an R package for summarizing and using the Portal Project Data","type":"publication"},{"authors":["Frank Pennekamp","Alison Iles","Joshua Garland","Georgina Brennan","Ulrich Brose","Ursula Gaedke","Ute Jacob","Pavel Kratina","Blake Matthews","Stephan Munch","Mark Novak","Gian Marco Palamara","Bjorn Rall","Benjamin Rosenbaum","Andrea Tabi","Colette Ward","Richard Williams","Hao Ye","Owen Petchey"],"categories":null,"content":"","date":1548201600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"93cbf54c0c056e6b5ac376fc44323ce5","permalink":"https://ha0ye.github.io/personal-website/publication/2019_intrinsic-predictability/","publishdate":"2019-01-23T00:00:00Z","relpermalink":"/personal-website/publication/2019_intrinsic-predictability/","section":"publication","summary":"Successfully predicting the future states of systems that are complex, stochastic and potentially chaotic is a major challenge. Model forecasting error (FE) is the usual measure of success; however model predictions provide no insights into the potential for improvement. In short, the realized predictability of a specific model is uninformative about whether the system is inherently predictable or whether the chosen model is a poor match for the system and our observations thereof. Ideally, model proficiency would be judged with respect to the systems’ *intrinsic* predictability — the highest achievable predictability given the degree to which system dynamics are the result of deterministic v. stochastic processes. Intrinsic predictability may be quantified with permutation entropy (PE), a model‐free, information‐theoretic measure of the complexity of a time series. By means of simulations we show that a correlation exists between estimated PE and FE and show how stochasticity, process error, and chaotic dynamics affect the relationship. This relationship is verified for a dataset of 461 empirical ecological time series. We show how deviations from the expected PE‐FE relationship are related to covariates of data quality and the nonlinearity of ecological dynamics. These results demonstrate a theoretically‐grounded basis for a model‐free evaluation of a system's intrinsic predictability. Identifying the gap between the intrinsic and realized predictability of time series will enable researchers to understand whether forecasting proficiency is limited by the quality and quantity of their data or the ability of the chosen forecasting model to explain the data. Intrinsic predictability also provides a model‐free baseline of forecasting proficiency against which modeling efforts can be evaluated.","tags":["Nonlinear Dynamics","Forecasting","Time Series","Dynamics"],"title":"The intrinsic predictability of ecological time series and its potential to guide forecasting","type":"publication"},{"authors":null,"categories":["R"],"content":"  Agenda Resources Installation and Setup Backup option  Data Formats Determine embedding dimension using Simplex Projection Identify nonlinearity using S-map  Multivariate Models Convergent Cross Mapping Surrogate Analysis with CCM Extra topics   These are the notes for the rEDM tutorial I gave at the November 13-15 Nonlinear Dynamics and Fisheries Workshop at the NMFS Southwest Fisheries Science Center in Santa Cruz.\nAgenda   Time     900-915 set up computers  915-930 data formats  930-945 simplex, plotting rho vs. E  945-1000 s-map, plotting rho vs. theta  1000-1030 coffee break  1030-1100 multivariate models  1100-1130 convergent cross mapping  1130-1200 Q \u0026amp; A     Resources Long-form text for this tutorial can be found at: https://ha0ye.github.io/rEDM/articles/rEDM.html\n Installation and Setup If you already have R (and RStudio is recommended, as well), you can install from CRAN:\ninstall.packages(\u0026quot;rEDM\u0026quot;) Backup option If you don’t have R or are unable to install packages, you can try using Binder to run an RStudio session in a web browser. (Note that it may take a while to load).\n  Data Formats rEDM uses basic R data structures as input. These include data.frames, 2-D matrices, and 1-D vectors. The latest version of rEDM on GitHub also has support for ts and mts data types (R’s built in time series and multivariate time series data types).\nIn general, rEDM tries to figure out what data column you want: * For functions that operate on a single time series, rEDM will work on both 1-D vectors or use the 2nd column of a data.frame or matrix. * For functions that operate on multiple time series, rEDM expects a data.frame or matrix, and can index by either column name, or by numerical index (starting with 1), with an optional argument to indicate whether the first column is a time index.\n Determine embedding dimension using Simplex Projection We begin by loading the rEDM package into R:\nlibrary(rEDM) Then, we can examine the tentmap data:\ndata(tentmap_del) str(tentmap_del) ## num [1:999] -0.0992 -0.6013 0.7998 -0.7944 0.798 ... By default, rEDM functions run using leave-one-out validation on the entire time series. We can change this by specifying the lib and pred arguments:\nlib \u0026lt;- c(1, 100) pred \u0026lt;- c(201, 500) Here, the first 100 points (positions 1 to 100) in the time series constitute the “library set”, and the last 300 points (positions 201 to 500) constitute the “prediction set”.\nThese separate sections are used to define the time-lagged vectors and one-step-ahead targets for forecasting.\nsimplex_output \u0026lt;- simplex(tentmap_del, lib, pred) str(simplex_output) ## \u0026#39;data.frame\u0026#39;: 10 obs. of 16 variables: ## $ E : int 1 2 3 4 5 6 7 8 9 10 ## $ tau : num 1 1 1 1 1 1 1 1 1 1 ## $ tp : num 1 1 1 1 1 1 1 1 1 1 ## $ nn : num 2 3 4 5 6 7 8 9 10 11 ## $ num_pred : num 299 298 297 296 295 294 293 292 291 290 ## $ rho : num 0.847 0.962 0.942 0.91 0.874 ... ## $ mae : num 0.207 0.102 0.138 0.19 0.235 ... ## $ rmse : num 0.392 0.187 0.236 0.291 0.334 ... ## $ perc : num 0.853 0.906 0.899 0.885 0.824 ... ## $ p_val : num 2.59e-102 4.99e-253 4.65e-199 1.54e-151 2.59e-118 ... ## $ const_pred_num_pred: num 299 298 297 296 295 294 293 292 291 290 ## $ const_pred_rho : num -0.668 -0.671 -0.671 -0.673 -0.671 ... ## $ const_pred_mae : num 1.02 1.02 1.02 1.02 1.01 ... ## $ const_pred_rmse : num 1.25 1.25 1.26 1.26 1.25 ... ## $ const_pred_perc : num 0.341 0.339 0.337 0.338 0.339 ... ## $ const_p_val : num 1 1 1 1 1 1 1 1 1 1 The output contains summary statistics for each run of the Simplex Projection model. By default, it uses values of 1 through 10 for E, the embedding dimension.\nWe can visualize the output by plotting rho vs E:\nplot(rho ~ E, data = simplex_output, type = \u0026quot;l\u0026quot;, xlab = \u0026quot;Embedding Dimension (E)\u0026quot;, ylab = \u0026quot;Forecast Skill (rho)\u0026quot;) With the observation that the best embedding dimension is 2, we can also look for how prediction decays with time to predict:\nsimplex_output \u0026lt;- simplex(tentmap_del, lib, pred, E = 2, tp = 1:10) plot(rho ~ tp, data = simplex_output, type = \u0026quot;l\u0026quot;, xlab = \u0026quot;Time to Prediction (tp)\u0026quot;, ylab = \u0026quot;Forecast Skill (rho)\u0026quot;) Identify nonlinearity using S-map smap_output \u0026lt;- s_map(tentmap_del, lib, pred, E = 2) str(smap_output) ## \u0026#39;data.frame\u0026#39;: 18 obs. of 17 variables: ## $ E : num 2 2 2 2 2 2 2 2 2 2 ... ## $ tau : num 1 1 1 1 1 1 1 1 1 1 ... ## $ tp : num 1 1 1 1 1 1 1 1 1 1 ... ## $ nn : num 0 0 0 0 0 0 0 0 0 0 ... ## $ theta : num 0e+00 1e-04 3e-04 1e-03 3e-03 1e-02 3e-02 1e-01 3e-01 5e-01 ... ## $ num_pred : num 298 298 298 298 298 298 298 298 298 298 ... ## $ rho : num 0.754 0.754 0.754 0.755 0.755 ... ## $ mae : num 0.363 0.363 0.363 0.363 0.363 ... ## $ rmse : num 0.451 0.451 0.451 0.451 0.451 ... ## $ perc : num 0.671 0.671 0.671 0.671 0.671 ... ## $ p_val : num 2.89e-64 2.86e-64 2.81e-64 2.61e-64 2.12e-64 ... ## $ const_pred_num_pred: num 298 298 298 298 298 298 298 298 298 298 ... ## $ const_pred_rho : num -0.671 -0.671 -0.671 -0.671 -0.671 ... ## $ const_pred_mae : num 1.02 1.02 1.02 1.02 1.02 ... ## $ const_pred_rmse : num 1.25 1.25 1.25 1.25 1.25 ... ## $ const_pred_perc : num 0.339 0.339 0.339 0.339 0.339 ... ## $ const_p_val : num 1 1 1 1 1 1 1 1 1 1 ... Here, the relevant parameter is theta, which is the nonlinear tuning parameter: a value of 0 corresponds to a linear AR model, and values greater than 0 cause the length-scale for the S-map to become smaller and smaller (i.e. the fitted function is allowed to be more “wiggly”)\nplot(rho ~ theta, data = smap_output, type = \u0026quot;l\u0026quot;, xlab = \u0026quot;Nonlinearity (theta)\u0026quot;, ylab = \u0026quot;Forecast Skill (rho)\u0026quot;) Here, we see that the forecast skill continues to improve with theta. This can be common for simulated data where there is no noise to the observation. In general, however, we would expect there to be tradeoff from allowing the S-map to be more and more wiggly and the number of data points that are used to determine the function.\nWe can test this by adding a bit of observational error to the time series and trying again:\ntentmap_noisy \u0026lt;- tentmap_del + rnorm(length(tentmap_del), sd = sd(tentmap_del) * 0.2) smap_output \u0026lt;- s_map(tentmap_noisy, lib, pred, E = 2) plot(rho ~ theta, data = smap_output, type = \u0026quot;l\u0026quot;, xlab = \u0026quot;Nonlinearity (theta)\u0026quot;, ylab = \u0026quot;Forecast Skill (rho)\u0026quot;)   Multivariate Models Although Takens’ theorem supports using lags of only 1 time series, including explicit coordinates from other time series in the system can produce better models, and maybe necessary if there are stochastic drivers.\nFor the data, we use a 3-species simulated Lotka-Volterra, that already has columns for lagged copies of the variables:\ndata(block_3sp) str(block_3sp) ## \u0026#39;data.frame\u0026#39;: 200 obs. of 10 variables: ## $ time : int 1 2 3 4 5 6 7 8 9 10 ... ## $ x_t : num -0.742 1.245 -1.918 -0.962 1.332 ... ## $ x_t-1: num NA -0.742 1.245 -1.918 -0.962 ... ## $ x_t-2: num NA NA -0.742 1.245 -1.918 ... ## $ y_t : num -1.268 1.489 -0.113 -1.107 2.385 ... ## $ y_t-1: num NA -1.268 1.489 -0.113 -1.107 ... ## $ y_t-2: num NA NA -1.268 1.489 -0.113 ... ## $ z_t : num -1.864 -0.482 1.535 -1.493 -1.119 ... ## $ z_t-1: num NA -1.864 -0.482 1.535 -1.493 ... ## $ z_t-2: num NA NA -1.864 -0.482 1.535 ... Next, we define our various input parameters. Because we explicitly set the coordinates for the embedding, we don’t specify the embedding dimension.\nlib \u0026lt;- c(1, 100) pred \u0026lt;- c(101, 200) cols \u0026lt;- c(1, 2, 4) target \u0026lt;- 1 When using the block_lnlp function, we want to be careful to indicate whether the first column in the data is a time index:\nblock_lnlp_output \u0026lt;- block_lnlp(block_3sp, lib = lib, pred = pred, columns = cols, target_column = target, first_column_time = TRUE) If we refer to columns by names instead of numerical index, we don’t need to specify first_column_time:\nblock_lnlp_output \u0026lt;- block_lnlp(block_3sp, lib = lib, pred = pred, columns = c(\u0026quot;x_t\u0026quot;, \u0026quot;x_t-1\u0026quot;, \u0026quot;y_t\u0026quot;), target_column = \u0026quot;x_t\u0026quot;) To retrieve the raw predictions, we need to tell rEDM functions to return more than just the statistical summaries:\nblock_lnlp_output \u0026lt;- block_lnlp(block_3sp, lib = lib, pred = pred, columns = c(\u0026quot;x_t\u0026quot;, \u0026quot;x_t-1\u0026quot;, \u0026quot;y_t\u0026quot;), target_column = \u0026quot;x_t\u0026quot;, stats_only = FALSE) This then allows us to pull out the observed and predicted values to plot. Note that we use the [[1]] indexing to pull out just the output from the first (and only) model that was run when calling block_lnlp().\nmodel_output \u0026lt;- block_lnlp_output$model_output[[1]] observed \u0026lt;- model_output$obs predicted \u0026lt;- model_output$pred plot_range \u0026lt;- range(c(observed, predicted), na.rm = TRUE) plot(observed, predicted, xlim = plot_range, ylim = plot_range, xlab = \u0026quot;Observed\u0026quot;, ylab = \u0026quot;Predicted\u0026quot;, asp = 1) abline(a = 0, b = 1, lty = 2, col = \u0026quot;blue\u0026quot;)  Convergent Cross Mapping Convergent cross mapping is a way to determine if two time series variables interact in a dynamic system. The procedure works by embedding the affected variable and trying to map back to the causal variable.\nHere, we’ll explore the example of sardine, anchovy, and sea-surface temperature from Deyle et al. (2013).\ndata(sardine_anchovy_sst) str(sardine_anchovy_sst) ## \u0026#39;data.frame\u0026#39;: 78 obs. of 5 variables: ## $ year : num 1929 1930 1931 1932 1933 ... ## $ anchovy: num -0.0076 -0.0096 -0.00844 -0.00835 -0.00775 ... ## $ sardine: num 1.77 -1.152 -1.421 0.112 1.516 ... ## $ sio_sst: num -0.35239 0.00115 1.06822 0.53186 -0.55206 ... ## $ np_sst : num -0.348 0.329 1.61 1.265 0.04 ... The convergence criterion of CCM means that the mapping relationship should get better with longer time series. We test that by taking subsamples of the data (randomly, with replacement):\nanchovy_xmap_sst \u0026lt;- ccm(sardine_anchovy_sst, E = 3, lib_column = \u0026quot;anchovy\u0026quot;, target_column = \u0026quot;np_sst\u0026quot;, lib_sizes = seq(10, 80, by = 10), num_samples = 100, random_libs = TRUE, replace = TRUE, silent = TRUE) sst_xmap_anchovy \u0026lt;- ccm(sardine_anchovy_sst, E = 3, lib_column = \u0026quot;np_sst\u0026quot;, target_column = \u0026quot;anchovy\u0026quot;, lib_sizes = seq(10, 80, by = 10), num_samples = 100, random_libs = TRUE, replace = TRUE, silent = TRUE) Just as with the previous functions, the output is one row for each model run (in this case, for each subsampling). To visualize the output, we want to compute the average effect at each subsample size:\na_xmap_t_means \u0026lt;- ccm_means(anchovy_xmap_sst) t_xmap_a_means \u0026lt;- ccm_means(sst_xmap_anchovy) plot(a_xmap_t_means$lib_size, pmax(0, a_xmap_t_means$rho), type = \u0026quot;l\u0026quot;, col = \u0026quot;red\u0026quot;, xlab = \u0026quot;Library Size\u0026quot;, ylab = \u0026quot;Cross Map Skill (rho)\u0026quot;, ylim = c(0, 0.25)) lines(t_xmap_a_means$lib_size, pmax(0, t_xmap_a_means$rho), col = \u0026quot;blue\u0026quot;) legend(x = \u0026quot;topleft\u0026quot;, legend = c(\u0026quot;anchovy xmap SST\u0026quot;, \u0026quot;SST xmap anchovy\u0026quot;), col = c(\u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;), lwd = 1, bty = \u0026quot;n\u0026quot;, inset = 0.02, cex = 0.8)  Surrogate Analysis with CCM To test for the significance of cross map effects, we use randomization tests with surrogate time series. The idea behind this is that the computed test statistic out of the CCM analysis is rho vs. lib_size for the actual time series, and we want to compare that with the rho vs. lib_size for a null model. rEDM has a built-in function, make_surrogate_data() to generate surrogate time series under different null models.\nThe procedure then, is as follows: * for the effect of X on Y, cross mapping is from Y to X * we want to know whether the recovered information about X is unique to the real data rather than just a statistical property of Y, so we generate surrogates of Y * compute cross mapping from surrogates of Y to the actual X * from the null distribution of multiple surrogates, we can pull out a 95% quantile for a significance test at alpha = 0.05\nHere is an example using the paramecium-didinium time series:\ndata(\u0026quot;paramecium_didinium\u0026quot;) str(paramecium_didinium) ## \u0026#39;data.frame\u0026#39;: 71 obs. of 3 variables: ## $ time : num 0 0.52 1.01 1.54 2.04 2.51 3 3.46 3.97 4.5 ... ## $ paramecium: num 15.7 53.6 73.3 93.9 115.4 ... ## $ didinium : num 5.76 9.05 17.26 41.97 55.97 ... ## Define a ccm function to use for the actual time series and surrogates ccm_func \u0026lt;- function(data, E = 3, lib_column = \u0026quot;paramecium\u0026quot;, target_column = \u0026quot;didinium\u0026quot;) { ccm(data, E = E, lib_column = lib_column, target_column = target_column, lib_sizes = seq(10, 71, by = 10), num_samples = 100, random_libs = TRUE, replace = FALSE, silent = TRUE) } ## CCM on actual time series ---- paramecium_xmap_didinium \u0026lt;- ccm_func(paramecium_didinium) para_xmap_didi_means \u0026lt;- ccm_means(paramecium_xmap_didinium) ## CCM on surrogate time series ---- # make surrogates para_surrogates \u0026lt;- make_surrogate_data(paramecium_didinium$paramecium) # for each surrogate time series # run CCM with the same parameters from that surrogate to SST # compute the mean rho in the same way # output is the null distribution for the cross map effect surrogate_output \u0026lt;- lapply(seq_len(NCOL(para_surrogates)), function(i) { surrogate_ts \u0026lt;- para_surrogates[, i] xmap_output \u0026lt;- ccm_func(cbind(surrogate_ts, paramecium_didinium$didinium), lib_column = 1, target_column = 2) xmap_means \u0026lt;- ccm_means(xmap_output) }) surrogate_means \u0026lt;- do.call(rbind, surrogate_output) surrogate_upper_95 \u0026lt;- ccm_means(surrogate_means, FUN = quantile, probs = 0.95) ## plot the output ---- plot(rho ~ lib_size, data = para_xmap_didi_means, type = \u0026quot;l\u0026quot;, col = \u0026quot;red\u0026quot;, ylim = c(0, 1), xlab = \u0026quot;Library Size\u0026quot;, ylab = \u0026quot;Cross Map Skill (rho)\u0026quot;) lines(surrogate_upper_95$lib_size, surrogate_upper_95$rho, lty = 2, col = \u0026quot;red\u0026quot;) legend(x = \u0026quot;topleft\u0026quot;, legend = c(\u0026quot;paramecium xmap didinium (actual)\u0026quot;, \u0026quot;paramecium xmap didinium (null 95%)\u0026quot;), col = \u0026quot;red\u0026quot;, lty = c(1, 2), bty = \u0026quot;n\u0026quot;, inset = 0.02, cex = 0.8)  Extra topics For examples related to: * examining S-map coefficients * time delay of causal effects in CCM * Gaussian Process implementation\nrefer to the online tutorial\n ","date":1542326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"f1122212ac5b3a02ff9e89d5d70811ce","permalink":"https://ha0ye.github.io/personal-website/post/2018-11-16-nmfs-redm-tutorial/","publishdate":"2018-11-16T00:00:00Z","relpermalink":"/personal-website/post/2018-11-16-nmfs-redm-tutorial/","section":"post","summary":"Agenda Resources Installation and Setup Backup option  Data Formats Determine embedding dimension using Simplex Projection Identify nonlinearity using S-map  Multivariate Models Convergent Cross Mapping Surrogate Analysis with CCM Extra topics   These are the notes for the rEDM tutorial I gave at the November 13-15 Nonlinear Dynamics and Fisheries Workshop at the NMFS Southwest Fisheries Science Center in Santa Cruz.\nAgenda   Time     900-915 set up computers  915-930 data formats  930-945 simplex, plotting rho vs.","tags":["rEDM","forecasting","fisheries","time series","nonlinear dynamics","causality"],"title":"rEDM Tutorial","type":"post"},{"authors":null,"categories":null,"content":"","date":1542153600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"6eed67417a64a0dde954b03cb48a9e36","permalink":"https://ha0ye.github.io/personal-website/project/portalr/","publishdate":"2018-11-14T00:00:00Z","relpermalink":"/personal-website/project/portalr/","section":"project","summary":"R package for downloading and summarizing the Portal data","tags":["Software","Data","R"],"title":"portalR","type":"project"},{"authors":null,"categories":null,"content":"","date":1542153600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"9386d8c99aad6efbace1abadf3976985","permalink":"https://ha0ye.github.io/personal-website/project/redm/","publishdate":"2018-11-14T00:00:00Z","relpermalink":"/personal-website/project/redm/","section":"project","summary":"R package for empirical dynamic modeling","tags":["Software","R","Time Series","Dynamics"],"title":"rEDM","type":"project"},{"authors":null,"categories":["R"],"content":" While on my visit to the University of Nebraska, Lincoln, I had the pleasure of taking over Chris Chizinski’s R class on Friday (2018-11-02).\nI demo’d a few things about setting up RStudio, using RStudio packages and the here package, and then walked through a workflow of doing data analysis, converting code into functions, and writing scripts and functions to be more accessible for readers.\nFor reference, here are my slides.\n","date":1541376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"035d0c6f01fa6023d13c742b2acbbea2","permalink":"https://ha0ye.github.io/personal-website/post/2018-11-05-advice-on-data-analysis-in-r/","publishdate":"2018-11-05T00:00:00Z","relpermalink":"/personal-website/post/2018-11-05-advice-on-data-analysis-in-r/","section":"post","summary":"While on my visit to the University of Nebraska, Lincoln, I had the pleasure of taking over Chris Chizinski’s R class on Friday (2018-11-02).\nI demo’d a few things about setting up RStudio, using RStudio packages and the here package, and then walked through a workflow of doing data analysis, converting code into functions, and writing scripts and functions to be more accessible for readers.\nFor reference, here are my slides.","tags":["usethis","here","R markdown","lintr","git","reproducible research"],"title":"Advice on Data Analysis in R","type":"post"},{"authors":null,"categories":["R"],"content":" For my rEDM package, I’ve been using the pkgdown package to build a website comprising all the documentation and vignettes, for easy reference from a web browser.\nThe normal workflow for this is something like:\nMake updates to the package. Run pkgdown::build_site() to generate the website files into a docs folder. Commit changes and upload to GitHub. Use GitHub Pages, configured to source the files from the docs folder on the master branch.  But we’re programmers, so why can’t we automate the steps of 2-4? Luckily, there is a guide for this already.\nOne issue that I ran into is that some of the documentation is built using the current package, which isn’t installed by default in the Travis CI instance. (i.e. when Travis runs through the R CMD CHECK, it installs the package temporarily, but that does not stick around) Thus, I ran into errors when pkgdown tried to compile my README.Rmd, which contained an example of using the package, as well as the vignettes.\nMy solution to this was simple, include a line before running pkgdown::build_site() that installs the package. I ended up with this following segment of my .travis.yml file:\nafter_success: - R CMD INSTALL . - Rscript -e \u0026#39;pkgdown::build_site()\u0026#39; and after changing over my GitHub Pages setting to look in the gh-pages branch, everything was set!\n","date":1540339200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"24f6d81d91524c2221e5fbf7ed33b526","permalink":"https://ha0ye.github.io/personal-website/post/2018-10-24-pkgdown-ci/","publishdate":"2018-10-24T00:00:00Z","relpermalink":"/personal-website/post/2018-10-24-pkgdown-ci/","section":"post","summary":"For my rEDM package, I’ve been using the pkgdown package to build a website comprising all the documentation and vignettes, for easy reference from a web browser.\nThe normal workflow for this is something like:\nMake updates to the package. Run pkgdown::build_site() to generate the website files into a docs folder. Commit changes and upload to GitHub. Use GitHub Pages, configured to source the files from the docs folder on the master branch.","tags":["R markdown","pkgdown","continuous integration","package","Travis CI"],"title":"Deploying package documentation with pkgdown and Travis CI","type":"post"},{"authors":null,"categories":["R"],"content":"  Motivation What I used to do Why use an R package?  How-to Guide Requirements Tutorial Setup Workflow Bonus steps  Other Readings   Motivation I’ve been wondering about the best way to organize (reproducible) research projects in R for a while now. I figured this might be a good spot to write up some thoughts.\nWhat I used to do Initially my projects would consist of just a few R files that separate out functions from a main script that calls the functions.\n# main.R library(XX) source(\u0026quot;functions.R\u0026quot;) if(FALSE) { clean_raw_data() run_analysis() make_figure_1() make_figure_2(\u0026quot;figure_2.pdf\u0026quot;) } This does a couple of things:\nI write code to do separate components of the analysis, then once I’ve tested it to see that it works, I can wrap it up in a function and hide most of the code away. Wrapping the calls inside an if(FALSE) block allows me to step through one function at a time in RStudio, or source the entire script to load dependencies and allow me to write new code with minimal effort. I wrap a lot of things like filenames and parameter values into the default arguments for functions. This allows me to call them plainly and have them work for the generic case, and also allows me to re-use the code for variations on the analysis or different data. In the case of figures, I usually have a default filename = NULL, which switches from plotting to the “Plots” window and plotting to a file.  For the most part this works pretty well, I can organize my data files and figure files into subfolders in an RStudio project, and put the whole thing in GitHub. Then anyone who wants to use any of the functions can source the R scripts pretty easily.\n Why use an R package? So then, why format a project as an R package? I see a few advantages:\nIt is a consistent way of organizing a project that is also familiar for people used to writing or navigating R packages. It clearly defines dependencies in one central location, which simplifies the workflow when writing multiple analyses (where one would need to load up all packages and defined functions). It is easier for anyone else to re-use components of the project. Sharing the project on GitHub, allows anyone to use devtools::install_github() to install dependencies, and enable loading of all the functions and datasets using library(). This also includes extensions or re-use by oneself. It lowers the barrier to good practices, like testing using the testthat framework. If the code is methodological, conversion to a more broadly available R package on CRAN is much simpler.    How-to Guide Requirements First, this assumes you have some familiarity with using R packages, RStudio projects, and so forth. If you’re just getting started with R, https://whattheyforgot.org/ is a great explanation on how to set up your workflow.\nWhat this assumes:\nYou have RStudio installed. I strongly recommend checking some options to always start with an empty workspace. You have git installed. Here are some nice installation instructions.   Tutorial Setup Install the devtools package from CRAN:  install.packages(\u0026quot;devtools\u0026quot;) Install the rrtools package from GitHub:  devtools::install_github(\u0026quot;benmarwick/rrtools\u0026quot;) Create a new project:  # I have a `projects` folder within my home directory where I store projects. # We\u0026#39;re going to call this project \u0026quot;demo\u0026quot;. Note that there are restrictions on # project names (letters, numbers, and periods only), and the folder name is set # to match the project name. rrtools::use_compendium(\u0026quot;~/projects/demo\u0026quot;) This will generate a bunch of files, and then re-open RStudio with the new project. All the subsequent commands are run from within that project.\nUse the MIT license:  # Use your name or organization name in place of \u0026quot;Hao Ye\u0026quot; (this will be the # entity to which copyright is assigned) usethis::use_mit_license(name = \u0026quot;Hao Ye\u0026quot;) Modify the DESCRIPTION file:  Package: demo Title: What the Package Does (One Line, Title Case) Version: 0.0.0.9000 Authors@R: person(given = \u0026quot;First\u0026quot;, family = \u0026quot;Last\u0026quot;, role = c(\u0026quot;aut\u0026quot;, \u0026quot;cre\u0026quot;), email = \u0026quot;first.last@example.com\u0026quot;) Description: What the package does (one paragraph) License: MIT + file LICENSE ByteCompile: true Encoding: UTF-8 LazyData: true Enable Git and make a first commit:  usethis::use_git() This will make an initial git commit, and then restart RStudio with the git pane enabled.\nEnable roxygen for documentation:  usethis::use_roxygen_md() Setup package dependencies:  If you use any other packages, include them as follows:\n# to install from CRAN usethis::use_package(\u0026quot;dplyr\u0026quot;) # to install from GitHub (assuming that\u0026#39;s where you\u0026#39;ve installed them from) usethis::use_dev_package(\u0026quot;portalr\u0026quot;) # to enable usage of the \u0026quot;pipe\u0026quot; (`%\u0026gt;%`) from `magrittr`: usethis::use_pipe() Setup the analysis folder (optional):  rrtools::use_analysis() This creates several folders to store manuscript outputs, data objects, figures, etc. Note that the R markdown template loads in the package we are currently working on, enabling all the functions and so forth to be used.\n Workflow Write code as functions in R files within the R folder:  # R/functions.R #\u0026#39; @export f \u0026lt;- function() {} At a minimum, you will want to include the special comment syntax as above so that the function is exported as a part of the package (i.e. someone can load the package and use the f() function.)\nMore info on writing documentation with Roxygen is available online in Hadley Wickham’s book on R packages.\nGenerate documentation and install the package:  To generate the documentation, use the keyboard shortcut in RStudio (CMD + SHIFT + D) or\ndevtools::document() To build and install the package and restart R, use the keyboard shortcut in RStudio (CMD + SHIFT + B) or the “Install and Restart” button from the “Build” pane:\nWrite and use the functions within R or R markdown files in the analysis folder. Follow your typical workflow for scripting R analyses or composing R markdown files.   Bonus steps Use tests:  usethis::use_testthat() Use Travis CI:  rrtools::use_travis() Use a Dockerfile:  rrtools::use_dockerfile() Use a R markdown readme:  rrtools::use_readme_rmd()   Other Readings rOpenSci’s notes on reproducible research compendiums The rrtools package. The usethis package. The R packages book.   ","date":1538784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"2493124148ae7c9bfeb1cdfaf3477d31","permalink":"https://ha0ye.github.io/personal-website/post/2018-10-06-project-setup/","publishdate":"2018-10-06T00:00:00Z","relpermalink":"/personal-website/post/2018-10-06-project-setup/","section":"post","summary":"Motivation What I used to do Why use an R package?  How-to Guide Requirements Tutorial Setup Workflow Bonus steps  Other Readings   Motivation I’ve been wondering about the best way to organize (reproducible) research projects in R for a while now. I figured this might be a good spot to write up some thoughts.\nWhat I used to do Initially my projects would consist of just a few R files that separate out functions from a main script that calls the functions.","tags":["R markdown","usethis","rrtools","package","reproducible research"],"title":"Guide for Using an R Package for Reproducible Research","type":"post"},{"authors":null,"categories":null,"content":"Under most definitions of \u0026ldquo;causality\u0026rdquo;, strict identification of causal interactions from observational data alone is impossible. However, it is possible to infer potential interactions among time series. Specifically, in dynamical systems, where time series are observations of the higher-dimensional behavior, methods from empirical Dynamic Modeling can be adapted to test whether two time series share information.\nThe essential idea is illustrated above. A causal interaction between $x$ and $z$ is inferred if there is shared dynamical information in the two time series. This is tested for by examining whether the attractor reconstructions of $x$ and $z$ are different representations of the same dynamics.\nWe described this idea initially in Sugihara et al. 2012. Later papers looked at the problem of combining data from replicate observations (see Clark et al. 2015) and expanding on the method to quantify time delays in causal effects (see van Nes et al. 2015 and Ye et al. 2015).\nFor a short video introduction, check out the following youtube video (part 3 of the longer playlist on EDM):   ","date":1536537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"41f91923fd39f57f7036817d3de899ce","permalink":"https://ha0ye.github.io/personal-website/project/causality/","publishdate":"2018-09-10T00:00:00Z","relpermalink":"/personal-website/project/causality/","section":"project","summary":"Identifying causal interactions among time series.","tags":["Dynamics","Time Series","Causality"],"title":"Causal inference","type":"project"},{"authors":null,"categories":null,"content":"Why is ecological forecasting a topic of interest? In some cases, advance knowledge about events (e.g. algal blooms) can be useful for responses or interventions, in some cases predicting population changes (e.g. of fish) is a critical component of management for sustainability.\nMore broadly, however, forecasting is a way of validating our models by testing predictions against observations. Importantly, while there are many studies about different mechanisms in ecology that may produce stable or unstable systems, much remains unclear about how natural ecosystems operate, especially in response to climate change.\nSome related papers:\n How much is the level of predictability of ecological time series attributable to data quality, modeling, and/or the intrinsic stochasticity of the underlying system? Pennekamp et al. 2019 \u0026ldquo;The intrinsic predictability of ecological time series and its potential to guide forecasting\u0026rdquo; Predicting algal blooms using models that incorporate biology and stochastic physical drivers. McGowan et al. 2017 \u0026ldquo;Predicting coastal algal blooms in southern California\u0026rdquo; How much ecological mechanism needs to be explicitly written out in equations to forecast? (not very much) Ye et al. 2015 \u0026ldquo;Equation-free mechanistic ecosystem forecasting using empirical dynamic modeling\u0026rdquo;  ","date":1536537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"be51639c91c4e53acb732eef563fb039","permalink":"https://ha0ye.github.io/personal-website/project/forecasting/","publishdate":"2018-09-10T00:00:00Z","relpermalink":"/personal-website/project/forecasting/","section":"project","summary":"Predicting the future state of ecosystems.","tags":["Forecasting","Time Series","Fisheries","Sunspots","Nonlinear Dynamics","Algal Blooms"],"title":"Forecasting","type":"project"},{"authors":null,"categories":null,"content":"Empirical dynamic modeling (EDM) is an approach for understanding the inherent dynamics (i.e. rules, processes, mechanisms) that underlie time series observations.\nFor example, in the graphic above, the time series is formed from the observation of variable $z$ of the Lorenz Attractor. More generally, time series can be more complex projections from the underlying dynamic system. EDM describes a way to recover information about the system from the time series.\nThe approach relies on the theory of attractor reconstruction (also referred to as \u0026ldquo;state-space reconstruction\u0026rdquo; or \u0026ldquo;time-delay embedding\u0026rdquo;) to transform time series into approximations of the original dynamic system that the time series are observations of. This has applications for forecasting, causal inference, and more.\nFor the R package for empirical dynamic modeling, check out rEDM.\nFor a short video introduction, check out the following youtube playlist (~6 minutes total):\n","date":1536192000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"386bfc1c1306c8d2072d6dbeffc80e5c","permalink":"https://ha0ye.github.io/personal-website/project/empirical-dynamic-modeling/","publishdate":"2018-09-06T00:00:00Z","relpermalink":"/personal-website/project/empirical-dynamic-modeling/","section":"project","summary":"Inferring system dynamics from time series","tags":["Time Series","Complexity","Stability","Nonlinear Dynamics","Forecasting","Causality"],"title":"Empirical Dynamic Modeling","type":"project"},{"authors":null,"categories":["R"],"content":"  Introduction Package Setup Data Forecasting Generate data to plot Figure   Introduction Since I’m writing R code to make certain figures for this website, I thought I could go ahead and annotate some of it in R markdown to serve as blog posts.\n Package Setup library(tidyverse) ## ── Attaching packages ─────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.2.1 ✔ purrr 0.3.2 ## ✔ tibble 2.1.3 ✔ dplyr 0.8.3 ## ✔ tidyr 1.0.0 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0 ## ── Conflicts ────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(rEDM) library(png) library(gganimate) # devtools::install_github(\u0026quot;thomasp85/gganimate\u0026quot;) # some packages that are installed, but which we\u0026#39;ll reference directly # library(portalr) # devtools::install_github(\u0026quot;weecology/portalr\u0026quot;) # library(forecast) # library(here)  Data First, we want to load up the data. Luckily the portalr package contains all we need to download and load up the rodent abundance time series we want.\nraw_dat \u0026lt;- portalr::abundance(shape = \u0026quot;flat\u0026quot;, # return data in long-form time = \u0026quot;all\u0026quot;, # return time in all formats clean = FALSE) # include data that hasn\u0026#39;t been QC\u0026#39;d ## Loading in data version 1.132.0 str(raw_dat) ## \u0026#39;data.frame\u0026#39;: 8967 obs. of 5 variables: ## $ newmoonnumber: int 28 28 28 28 28 28 28 28 28 28 ... ## $ period : num 27 27 27 27 27 27 27 27 27 27 ... ## $ censusdate : Date, format: \u0026quot;1979-09-22\u0026quot; \u0026quot;1979-09-22\u0026quot; ... ## $ species : Factor w/ 21 levels \u0026quot;BA\u0026quot;,\u0026quot;DM\u0026quot;,\u0026quot;DO\u0026quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ abundance : int 0 16 0 11 1 6 12 0 2 2 ... ## - attr(*, \u0026quot;na.action\u0026quot;)= \u0026#39;omit\u0026#39; Named int 1 2 3 4 5 6 7 8 9 10 ... ## ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; ... Next, we want to do some initial processing of the data. Here are the steps:\nfilter for just the Dipodomys merriami (species code = \u0026quot;DM\u0026quot;) linearly interpolate the dates and abundances for missing censuses (which are nearly-monthly) re-format the columns correctly  dat \u0026lt;- raw_dat %\u0026gt;% filter(species == \u0026quot;DM\u0026quot;) %\u0026gt;% select(-species, -period) %\u0026gt;% complete(newmoonnumber = full_seq(newmoonnumber, 1), fill = list(NA)) %\u0026gt;% mutate_at(vars(-newmoonnumber), forecast::na.interp) %\u0026gt;% mutate(censusdate = as.Date(as.numeric(censusdate), origin = \u0026quot;1970-01-01\u0026quot;), abundance = as.numeric(abundance)) str(dat) ## Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 494 obs. of 3 variables: ## $ newmoonnumber: num 28 29 30 31 32 33 34 35 36 37 ... ## $ censusdate : Date, format: \u0026quot;1979-09-22\u0026quot; \u0026quot;1979-10-24\u0026quot; ... ## $ abundance : num 16 15 25 30 35 19 23 22 23 21 ... Here’s an initial plot of the abundance:\nggplot(dat, aes(x = censusdate, y = abundance)) + geom_line() + theme_bw(base_size = 20, base_line_size = 1)  Figure 1: DM abundance through time   Forecasting For our next step, we’re going to use the functions in rEDM to fit some simple time series models to the abundance of DM through time using simple lags. We’ll go through some steps to determine the values for the hyper-parameters, E, and theta.\nFirst, using simplex() and picking the value of E based on the best fit.\ndm_count \u0026lt;- dat$abundance # pull out just the numeric vector simplex_out \u0026lt;- simplex(dm_count, E = 1:24, silent = TRUE) best_E \u0026lt;- simplex_out$E[which.min(simplex_out$rmse)] Next, using s_map() with E = 8 and picking the value of theta based on the best fit.\nsmap_out \u0026lt;- s_map(dm_count, E = best_E, silent = TRUE) best_theta \u0026lt;- smap_out$theta[which.min(smap_out$rmse)] Finally, re-running the S-map model again with E = 8 and theta = 0.5 to get a one-step ahead forecast for the future.\nout \u0026lt;- s_map(c(dm_count, NA), E = best_E, theta = best_theta, stats_only = FALSE) ## Warning in model$run(): Found overlap between lib and pred. Enabling cross- ## validation with exclusion radius = 0. forecast \u0026lt;- tail(out$model_output[[1]], 2)[1,] # get second-to-last row of model_output  Generate data to plot There are a few things we want to eventually plot, so we’ll need to construct the data appropriately.\nFirst, get the last values from the original data, and append the forecast. Here, we want to capture the uncertainty, so compute the standard-deviation as the square-root of the estimated variance.\nto_plot \u0026lt;- tail(dat) to_plot$sd \u0026lt;- 0 to_plot \u0026lt;- rbind(to_plot, data.frame(newmoonnumber = max(to_plot$newmoonnumber) + 1, censusdate = max(to_plot$censusdate) + 29, abundance = forecast$pred, sd = sqrt(forecast$pred_var))) Next, generate a sample of forecasts from the distribution, and create a data.frame that includes each forecast as a line from the last observation to the forecast.\nset.seed(123) num_forecasts \u0026lt;- 20 forecast_dist \u0026lt;- rnorm(num_forecasts, mean = forecast$pred, sd = sqrt(forecast$pred_var)) forecast_df \u0026lt;- map_dfr(seq(num_forecasts), function(idx) { temp \u0026lt;- to_plot # copy to_plot temp$abundance[NROW(temp)] \u0026lt;- forecast_dist[idx] temp$idx \u0026lt;- idx return(tail(temp, 2)) }) Finally, read in a silhouette picture of the rodent in question. We downloaded the appropriate image from the PortalPredictions repo, and added transparency and resized accordingly.\ndm_image \u0026lt;- readPNG(here::here(\u0026quot;static/img/dipodomys_merriami.png\u0026quot;))  Figure Ok, so we can put this all together now into a single figure.\nBecause of the background image, we’ll want to do pre-compute the plot limits and use those to position the image to ensure that it’s sized correctly.\nxlims \u0026lt;- range(to_plot$censusdate) ylims \u0026lt;- c(0, 60) We’ll use ggplot to assemble the figure, with a line for each forecast, adding a geom_ribbon for the +/- 2 SD for forecasts, a plain geom_line for the observations, and the rodent image as a background.\np \u0026lt;- ggplot(forecast_df, aes(x = censusdate, y = abundance)) + # plot boundaries coord_cartesian(xlim = xlims, ylim = ylims) + # background image annotation_raster(dm_image * 0.3, xmin = min(xlims), xmax = max(xlims), ymin = 9.5, ymax = 47, interpolate = TRUE) + # forecasts geom_line(size = 2, color = \u0026quot;blue\u0026quot;) + # observations (exclude last point, which is the forecast) geom_line(size = 2, data = to_plot[-NROW(to_plot), ]) + # uncertainty area geom_ribbon(aes(ymin = abundance - 2 * sd, ymax = abundance + 2 * sd), data = to_plot, fill = \u0026quot;#BBBBFF\u0026quot;, alpha = 0.5) + # theming theme_bw(base_size = 20, base_line_size = 1) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;DM Abundance\u0026quot;) To animate, we’ll use animate(), with some arguments for how to change each frame and saving the output as a gif.\nanimate(p + transition_time(idx), width = 400, height = 400) anim_save(here::here(\u0026quot;static/img/forecasting-preview.gif\u0026quot;))  ","date":1535932800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"1d99ed18f8d666027fc7508904a1ac60","permalink":"https://ha0ye.github.io/personal-website/post/2018-09-03-gganimate/","publishdate":"2018-09-03T00:00:00Z","relpermalink":"/personal-website/post/2018-09-03-gganimate/","section":"post","summary":"Introduction Package Setup Data Forecasting Generate data to plot Figure   Introduction Since I’m writing R code to make certain figures for this website, I thought I could go ahead and annotate some of it in R markdown to serve as blog posts.\n Package Setup library(tidyverse) ## ── Attaching packages ─────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.2.1 ✔ purrr 0.3.2 ## ✔ tibble 2.1.3 ✔ dplyr 0.","tags":["R Markdown","ggplot","gganimate","Forecasting"],"title":"Making gifs in R with `gganimate`","type":"post"},{"authors":null,"categories":null,"content":"One classical approach for stability involves studying the species interaction matrix, and determining whether the matrix produces asympotitic stability near an equilibrium. In Ushio et al. 2018, we extended this framework by using empirical dynamic modeling to quantify how the species interaction matrix changes over time. This then allows for estimation of stability as a changing quantity, or \u0026ldquo;dynamic stability\u0026rdquo;.\nUshio et al. 2018 describes and demonstrates the approach for a fish community in Maizuru Bay. Currently, I am investigating whether dynamic stability can be a useful guide for identifying abrupt shifts in ecological communities. Specifically, whether the approach can give insight into change in the rodent community at the Portal experimental site, and validating the method against the known shifts identified in Christensen et al. 2018.\nCode and data for this project are available on GitHub and initial results were presented at the 2018 ESA meeting in New Orleans.\n","date":1535846400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"9313f409f41bbcd7410376489478301e","permalink":"https://ha0ye.github.io/personal-website/project/dynamic-stability/","publishdate":"2018-09-02T00:00:00Z","relpermalink":"/personal-website/project/dynamic-stability/","section":"project","summary":"Dynamic quantification of stability in ecosystems.","tags":["Dynamic Stability","Time Series","Complexity","Stability"],"title":"Dynamic stability","type":"project"},{"authors":["George Sugihara","Keith R. Criddle","Mac McQuown","Alfredo Giron-Nava","Ethan Deyle","Adrienne Lee","Gerald Pao","Chase James","Erik Saberski","Hao Ye"],"categories":null,"content":"","date":1529020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"a5766c9efca6f15723658b46c939c7ab","permalink":"https://ha0ye.github.io/personal-website/publication/2018_bycatch-credits/","publishdate":"2018-06-15T00:00:00Z","relpermalink":"/personal-website/publication/2018_bycatch-credits/","section":"publication","summary":"After record salmon bycatch in 2007 by the Eastern Bering Sea and Aleutian Islands fishery for walleye Pollock, the North Pacific Fishery Management Council (NPFMC) concluded that additional management strategies were necessary to further control salmon bycatch. The Preliminary Preferred Alternative (PPA) was selected in April 2009 and implemented in January 2011 as Amendment 91. In this paper, we present the original comprehensive bycatch credits allocation and trading plan as designed by the first author as commissioned by the Alaskan Pollock Fleet for Chinook salmon, the Comprehensive Incentive Plan (CIP). The CIP, which uses individual (vessel-level) tradable encounter credits (ITEC), included incentives that make up the backbone of Amendment 91/PPA. While salmon bycatch has been reduced since the implementation of the PPA, the current amendment does not have individual vessel incentives that vary with the vulnerability of salmon populations. The CIP approach presented here provides robust vessel-level incentives to reduce Chinook salmon bycatch under all levels of salmon abundance, but particularly when salmon populations are at their lowest levels and are most vulnerable. The specific financial incentive structure in the full plan, with trading of by-catch liabilities among vessels, can be applied well in other fisheries where bycatch threatens both sustainability and profitability.","tags":["Fisheries","Fisheries Management","Bycatch"],"title":"Comprehensive incentives for reducing Chinook salmon bycatch in the Bering Sea walleye Pollock fishery: Individual tradable encounter credits","type":"publication"},{"authors":["Ethan Deyle","Amy M Schueller","Hao Ye","Gerald M Pao","George Sugihara"],"categories":null,"content":"","date":1526256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"1ead1cb4e2d28e2e5b641efd200d5093","permalink":"https://ha0ye.github.io/personal-website/publication/2018_menhaden-forecasts/","publishdate":"2018-05-14T00:00:00Z","relpermalink":"/personal-website/publication/2018_menhaden-forecasts/","section":"publication","summary":"Gulf (*Brevoortia patronus*, Clupeidae) and Atlantic menhaden (*Brevoortia tyrannus*, Clupeidae) support large fisheries that have shown substantial variability over several decades, in part, due to dependence on annual recruitment. Nevertheless, traditional stock–recruitment relationships lack predictive power for these stocks. Current management of Atlantic menhaden explicitly treats recruitment as a random process. However, traditional methods for understanding recruitment variability carry the very specific hypothesis that the effect of adult biomass on subsequent recruitment occurs independently of other ecosystem factors such as food availability and predation. Here, we evaluate the predictability of menhaden recruitment using a model‐free approach that is not restricted by these strong assumptions. We find that menhaden recruitment is predictable, but only when allowing for interdependence of stock with other ecological factors. Moreover, while the analysis confirms the presence of environmental effects, the environment alone does not readily account for the complexity of menhaden recruitment dynamics. The findings set the stage for revisiting recruitment prediction in management and serve as an instructive example in the ongoing debate about how to best treat and understand recruitment variability across species and fisheries.","tags":["Fisheries","Forecasting","Time Series"],"title":"Ecosystem‐based forecasts of recruitment in two menhaden species","type":"publication"},{"authors":["Masayuki Ushio","Chih-hao Hsieh","Reiji Masuda","Ethan R Deyle","Hao Ye","Chun-Wei Chang","George Sugihara","Michio Kondoh"],"categories":null,"content":"","date":1518652800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"d42410b6293780ce54eb532df0dfb03b","permalink":"https://ha0ye.github.io/personal-website/publication/2018_maizuru-dynamic-stability/","publishdate":"2018-02-15T00:00:00Z","relpermalink":"/personal-website/publication/2018_maizuru-dynamic-stability/","section":"publication","summary":"Ecological theory suggests that large-scale patterns such as community stability can be influenced by changes in interspecific interactions that arise from the behavioural and/or physiological responses of individual species varying over time[1–3]. Although this theory has experimental support[2,4,5], evidence from natural ecosystems is lacking owing to the challenges of tracking rapid changes in interspecific interactions (known to occur on timescales much shorter than a generation time)[6] and then identifying the effect of such changes on large-scale community dynamics. Here, using tools for analysing nonlinear time series[6–9] and a 12-year-long dataset of fortnightly collected observations on a natural marine fish community in Maizuru Bay, Japan, we show that short-term changes in interaction networks influence overall community dynamics. Among the 15 dominant species, we identify 14 interspecific interactions to construct a dynamic interaction network. We show that the strengths, and even types, of interactions change with time; we also develop a time-varying stability measure based on local Lyapunov stability for attractor dynamics in non-equilibrium nonlinear systems. We use this dynamic stability measure to examine the link between the time-varying interaction network and community stability. We find seasonal patterns in dynamic stability for this fish community that broadly support expectations of current ecological theory. Specifically, the dominance of weak interactions and higher species diversity during summer months are associated with higher dynamic stability and smaller population fluctuations. We suggest that interspecific interactions, community network structure and community stability are dynamic properties, and that linking fluctuating interaction networks to community-level dynamic properties is key to understanding the maintenance of ecological communities in nature.","tags":["Networks","Time Series","Stability","Dynamic Stability","Dynamics"],"title":"Fluctuating interaction network and time-varying stability of a natural fish community","type":"publication"},{"authors":["Anastasios A. Tsonis","Ethan R. Deyle","Hao Ye","George Sugihara"],"categories":null,"content":"","date":1507680000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"819b9ced3bf5b3c1d8cf8baf26338d9d","permalink":"https://ha0ye.github.io/personal-website/publication/2018_ccm-chapter/","publishdate":"2017-10-11T00:00:00Z","relpermalink":"/personal-website/publication/2018_ccm-chapter/","section":"publication","summary":"In this review paper we present the basic principles behind convergent cross mapping, a new causality detection method, as well as an example to demonstrate it.","tags":["Causality","Time Series"],"title":"Convergent Cross Mapping: Theory and an Example","type":"publication"},{"authors":["Alfredo Giron-Nava","Chase C. James","Andrew F. Johnson","David Dannecker","Bethany Kolody","Adrienne Lee","Maitreyi Nagarkar","Gerald M. Pao","Hao Ye","David G. Johns","George Sugihara"],"categories":null,"content":"","date":1496188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"aee9866a0f8cb625a2895e9b7a85927c","permalink":"https://ha0ye.github.io/personal-website/publication/2017_long-term-monitoring/","publishdate":"2017-05-31T00:00:00Z","relpermalink":"/personal-website/publication/2017_long-term-monitoring/","section":"publication","summary":"Although it seems obvious that with more data, the predictive capacity of ecological models should improve, a way to demonstrate this fundamental result has not been so obvious. In particular, when the standard models themselves are inadequate (von Bertalanffy, extended Ricker etc.) no additional data will improve performance. By using time series from the Sir Alister Hardy Foundation for Ocean Science Continuous Plankton Recorder, we demonstrate that long- term observations reveal both the prevalence of nonlinear processes in species abundances and an improvement in out-of-sample predictability as the number of observations increase. The empirical results presented here quantitatively demonstrate the importance of long-term temporal data collection programs for improving ecosystem models and forecasts, and to better support environmental management actions.","tags":["Nonlinear Dynamics","Time Series"],"title":"Quantitative argument for long-term ecological monitoring","type":"publication"},{"authors":["John A. McGowan","Ethan R. Deyle","Hao Ye","Melissa L. Carter","Charles T. Perretti","Kerri D. Seger","Alain de Verneil","George Sugihara"],"categories":null,"content":"","date":1490918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"d3b970617677bdee294dae9ae2d9b7ad","permalink":"https://ha0ye.github.io/personal-website/publication/2017_predicting-algal-blooms/","publishdate":"2017-03-31T00:00:00Z","relpermalink":"/personal-website/publication/2017_predicting-algal-blooms/","section":"publication","summary":"The irregular appearance of planktonic algae blooms off the coast of southern California has been a source of wonder for over a century. Although large algal blooms can have significant negative impacts on ecosystems and human health, a predictive understanding of these events has eluded science, and many have come to regard them as ultimately random phenomena. However, the highly nonlinear nature of ecological dynamics can give the appearance of randomness and stress traditional methods—such as model fitting or analysis of variance—to the point of breaking. The intractability of this problem from a classical linear standpoint can thus give the impression that algal blooms are fundamentally unpredictable. Here, we use an exceptional time series study of coastal phytoplankton dynamics at La Jolla, CA, with an equation-free modeling approach, to show that these phenomena are not random, but can be understood as nonlinear population dynamics forced by external stochastic drivers (so-called 'stochastic chaos'). The combination of this modeling approach with an extensive dataset allows us to not only describe historical behavior and clarify existing hypotheses about the mechanisms, but also make out-of-sample predictions of recent algal blooms at La Jolla that were not included in the model development.","tags":["Nonlinear Dynamics","Forecasting","Algal Blooms","Time Series"],"title":"Predicting coastal algal blooms in southern California","type":"publication"},{"authors":["George Sugihara","Ethan R Deyle","Hao Ye"],"categories":null,"content":"","date":1489536000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"8c846f1b8e04bef8eb66b8e7898ab554","permalink":"https://ha0ye.github.io/personal-website/publication/2017_reply-baskerville-cobey/","publishdate":"2017-03-15T00:00:00Z","relpermalink":"/personal-website/publication/2017_reply-baskerville-cobey/","section":"publication","summary":"","tags":["Causality","Epidemiology","Time Series"],"title":"Reply to Baskerville and Cobey: Misconceptions about causation with synchrony and seasonal drivers","type":"publication"},{"authors":["Laura S. Storch","Sarah M. Glaser","Hao Ye","Andrew A. Rosenberg"],"categories":null,"content":"","date":1487116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"a55f845909d7c2f1d6dc82a5bbe526ba","permalink":"https://ha0ye.github.io/personal-website/publication/2017_nonlinearity/","publishdate":"2017-02-15T00:00:00Z","relpermalink":"/personal-website/publication/2017_nonlinearity/","section":"publication","summary":"Although all models are simplified approximations of reality, they remain useful tools for understanding, predicting, and managing populations and ecosystems. However, a model’s utility is contingent on its suitability for a given task. Here, we examine two model types: single-species fishery stock assessment and multispecies marine ecosystem models. Both are efforts to predict trajectories of populations and ecosystems to inform fisheries management and conceptual understanding. However, many of these ecosystems exhibit nonlinear dynamics, which may not be represented in the models. As a result, model outputs may underestimate variability and overestimate stability. Using nonlinear forecasting methods, we compare predictability and nonlinearity of model outputs against model inputs using data and models for the California Current System. Compared with model inputs, time series of model-processed outputs show more predictability but a higher prevalence of linearity, suggesting that the models misrepresent the actual predictability of the modeled systems. Thus, caution is warranted: using such models for management or scenario exploration may produce unforeseen consequences, especially in the context of unknown future impacts.","tags":["Nonlinear Dynamics","Fisheries","Time Series","Dynamics"],"title":"Stock assessment and end-to-end ecosystem models alter dynamics of fisheries data","type":"publication"},{"authors":["Hao Ye","George Sugihara"],"categories":null,"content":"","date":1472169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"2bd073c4192285e10fc4a9feae0fd8f7","permalink":"https://ha0ye.github.io/personal-website/publication/2016_information-leverage/","publishdate":"2016-08-26T00:00:00Z","relpermalink":"/personal-website/publication/2016_information-leverage/","section":"publication","summary":"In ecological analysis, complexity has been regarded as an obstacle to overcome. Here we present a straightforward approach for addressing complexity in dynamic interconnected systems. We show that complexity, in the form of multiple interacting components, can actually be an asset for studying natural systems from temporal data. The central idea is that multidimensional time series enable system dynamics to be reconstructed from multiple viewpoints, and these viewpoints can be combined into a single model. We show how our approach, multiview embedding (MVE), can improve forecasts for simulated ecosystems and a mesocosm experiment. By leveraging complexity, MVE is particularly effective for overcoming the limitations of short and noisy time series and should be highly relevant for many areas of science.","tags":["Complexity","Dynamics","Time Series"],"title":"Information leverage in interconnected ecosystems: Overcoming the curse of dimensionality","type":"publication"},{"authors":["Hao Ye","Ethan Deyle","George Sugihara"],"categories":null,"content":"","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"ab75d19fac86fa310c89987976d99420","permalink":"https://ha0ye.github.io/personal-website/publication/2015_nonlinear-prediction/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/personal-website/publication/2015_nonlinear-prediction/","section":"publication","summary":"","tags":["Nonlinear Dynamics","Forecasting","Time Series"],"title":"Predicting the future in a nonlinear world","type":"publication"},{"authors":["Hao Ye","Ethan R. Deyle","Luis J. Gilarranz","George Sugihara"],"categories":null,"content":"","date":1444003200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"f0913af8827611a171b74de089f2563b","permalink":"https://ha0ye.github.io/personal-website/publication/2015_ccm-time-delays/","publishdate":"2015-10-05T00:00:00Z","relpermalink":"/personal-website/publication/2015_ccm-time-delays/","section":"publication","summary":"An important problem across many scientific fields is the identification of causal effects from observational data alone. Recent methods (convergent cross mapping, CCM) have made substantial progress on this problem by applying the idea of nonlinear attractor reconstruction to time series data. Here, we expand upon the technique of CCM by explicitly considering time lags. Applying this extended method to representative examples (model simulations, a laboratory predator-prey experiment, temperature and greenhouse gas reconstructions from the Vostok ice core, and long-term ecological time series collected in the Southern California Bight), we demonstrate the ability to identify different time-delayed interactions, distinguish between synchrony induced by strong unidirectional-forcing and true bidirectional causality, and resolve transitive causal chains.","tags":["Causality","Time Series"],"title":"Distinguishing time-delayed causal interactions using convergent cross mapping","type":"publication"},{"authors":["Hao Ye","George Sugihara","Ethan R. Deyle","Robert M. May","Kyle Swanson","Anastasios A. Tsonis"],"categories":null,"content":"","date":1440460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"15cf53abe44df170dd44db952b672494","permalink":"https://ha0ye.github.io/personal-website/publication/2015_reply-luo/","publishdate":"2015-08-25T00:00:00Z","relpermalink":"/personal-website/publication/2015_reply-luo/","section":"publication","summary":"","tags":["Causality","Climate","Time Series"],"title":"Reply to Luo et al.: Robustness of causal effects of galactic cosmic rays on interannual variation in global temperature","type":"publication"},{"authors":["Adam Thomas Clark","Hao Ye","Forest Isbell","Ethan R. Deyle","Jane Cowles","G. David Tilman","George Sugihara"],"categories":null,"content":"","date":1430438400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"4e4eb81e4e937aa57ebb2ae7ddae75df","permalink":"https://ha0ye.github.io/personal-website/publication/2015_spatial-ccm/","publishdate":"2015-05-01T00:00:00Z","relpermalink":"/personal-website/publication/2015_spatial-ccm/","section":"publication","summary":"Recent developments in complex systems analysis have led to new techniques for detecting causal relationships using relatively short time series, on the order of 30 sequential observations. Although many ecological observation series are even shorter, perhaps fewer than ten sequential observations, these shorter time series are often highly replicated in space (i.e., plot replication). Here, we combine the existing techniques of convergent cross mapping (CCM) and dewdrop regression to build a novel test of causal relations that leverages spatial replication, which we call multispatial CCM. Using examples from simulated and real‐world ecological data, we test the ability of multispatial CCM to detect causal relationships between processes. We find that multispatial CCM successfully detects causal relationships with as few as five sequential observations, even in the presence of process noise and observation error. Our results suggest that this technique may constitute a useful test for causality in systems where experiments are difficult to perform and long time series are not available. This new technique is available in the multispatialCCM package for the R programming language.","tags":["Causality","Time Series","Software"],"title":"Spatial convergent cross mapping to detect causal relationships from short time series","type":"publication"},{"authors":["Hao Ye","Richard J. Beamish","Sarah M. Glaser","Sue C. H. Grant","Chih-hao Hsieh","Laura J. Richards","Jon T. Schnute","George Sugihara"],"categories":null,"content":"","date":1427760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"cce12eecea50385bd5f0bd67eab52ae0","permalink":"https://ha0ye.github.io/personal-website/publication/2015_equation-free-modeling/","publishdate":"2015-03-31T00:00:00Z","relpermalink":"/personal-website/publication/2015_equation-free-modeling/","section":"publication","summary":"It is well known that current equilibrium-based models fall short as predictive descriptions of natural ecosystems, and particularly of fisheries systems that exhibit nonlinear dynamics. For example, model parameters assumed to be fixed constants may actually vary in time, models may fit well to existing data but lack out-of-sample predictive skill, and key driving variables may be misidentified due to transient (mirage) correlations that are common in nonlinear systems. With these frailties, it is somewhat surprising that static equilibrium models continue to be widely used. Here, we examine empirical dynamic modeling (EDM) as an alternative to imposed model equations and that accommodates both nonequilibrium dynamics and nonlinearity. Using time series from nine stocks of sockeye salmon (Oncorhynchus nerka) from the Fraser River system in British Columbia, Canada, we perform, for the the first time to our knowledge, real-data comparison of contemporary fisheries models with equivalent EDM formulations that explicitly use spawning stock and environmental variables to forecast recruitment. We find that EDM models produce more accurate and precise forecasts, and unlike extensions of the classic Ricker spawner–recruit equation, they show significant improvements when environmental factors are included. Our analysis demonstrates the strategic utility of EDM for incorporating environmental influences into fisheries forecasts and, more generally, for providing insight into how environmental factors can operate in forecast models, thus paving the way for equation-free mechanistic forecasting to be applied in management contexts.","tags":["Fisheries","Forecasting","Time Series","Nonlinear Dynamics"],"title":"Equation-free mechanistic ecosystem forecasting using empirical dynamic modeling","type":"publication"},{"authors":["Egbert H. van Nes","Marten Scheffer","Victor Brovkin","Timothy M. Lenton","Hao Ye","Ethan Deyle","George Sugihara"],"categories":null,"content":"","date":1427673600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"0357473d88f8babb7764855997d98258","permalink":"https://ha0ye.github.io/personal-website/publication/2015_causality-climate-change/","publishdate":"2015-03-30T00:00:00Z","relpermalink":"/personal-website/publication/2015_causality-climate-change/","section":"publication","summary":"The statistical association between temperature and greenhouse gases over glacial cycles is well documented[1], but causality behind this correlation remains difficult to extract directly from the data. A time lag of CO_2 behind Antarctic temperature—originally thought to hint at a driving role for temperature[2,3]—is absent[4,5] at the last deglaciation, but recently confirmed at the last ice age inception[6] and the end of the earlier termination II (ref. 7). We show that such variable time lags are typical for complex nonlinear systems such as the climate, prohibiting straightforward use of correlation lags to infer causation. However, an insight from dynamical systems theory[8] now allows us to circumvent the classical challenges of unravelling causation from multivariate time series. We build on this insight to demonstrate directly from ice-core data that, over glacial–interglacial timescales, climate dynamics are largely driven by internal Earth system mechanisms, including a marked positive feedback effect from temperature variability on greenhouse-gas concentrations.","tags":["Causality","Climate","Time Series","Nonlinear Dynamics"],"title":"Causal feedbacks in climate change","type":"publication"},{"authors":["Sarah M Glaser","Michael J Fogarty","Hui Liu","Irit Altman","Chih‐Hao Hsieh","Les Kaufman","Alec D MacCall","Andrew A Rosenberg","Hao Ye","George Sugihara"],"categories":null,"content":"","date":1417392000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"be17d785e01fbb8c107fa6eb284b8ca0","permalink":"https://ha0ye.github.io/personal-website/publication/2014_complex-dynamics-fisheries/","publishdate":"2014-12-01T00:00:00Z","relpermalink":"/personal-website/publication/2014_complex-dynamics-fisheries/","section":"publication","summary":"Complex nonlinear dynamics in marine fisheries create challenges for prediction and management, yet the extent to which they occur in fisheries is not well known. Using nonlinear forecasting models, we analysed over 200 time series of survey abundance and landings from two distinct ecosystems for patterns of dynamic complexity (dimensionality and nonlinear dynamics) and predictability. Differences in system dimensionality and nonlinear dynamics were associated with time series that reflected human intervention via fishing effort, implying the coupling between human and natural systems generated dynamics distinct from those detected in the natural resource subsystem alone. Estimated dimensionality was highest for landings and higher in abundance indices of unfished species than fished species. Fished species were more likely to display nonlinear dynamics than unfished species, and landings were significantly less predictable than abundance indices. Results were robust to variation in life history characteristics. Dynamics were predictable over a 1‐year time horizon in seventy percent of time series, but predictability declined exponentially over a 5‐year horizon. The ability to make predictions in fisheries systems is therefore extremely limited. To our knowledge, this is the first cross‐system comparative study, and the first at the scale of individual species, to analyse empirically the dynamic complexity observed in fisheries data and to quantify predictability broadly. We outline one application of short‐term forecasts to a precautionary approach to fisheries management that could improve how uncertainty and forecast error are incorporated into assessment through catch limit buffers.","tags":["Complexity","Nonlinear Dynamics","Fisheries","Time Series","Dynamics"],"title":"Complex dynamics may limit prediction in marine fisheries","type":"publication"},{"authors":["Hui Liu","Michael J. Fogarty","Jonathan A. Hare","Chih-hao Hsieh","Sarah M. Glaser","Hao Ye","Ethan Deyle","George Sugihara"],"categories":null,"content":"","date":1393632000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"63900368e2b96451c0e53c1dad562503","permalink":"https://ha0ye.github.io/personal-website/publication/2014_zooplankton-fish-environment/","publishdate":"2014-03-01T00:00:00Z","relpermalink":"/personal-website/publication/2014_zooplankton-fish-environment/","section":"publication","summary":"The dynamics of marine fishes are closely related to lower trophic levels and the environment. Quantitatively understanding ecosystem dynamics linking environmental variability and prey resources to exploited fishes is crucial for ecosystem-based management of marine living resources. However, standard statistical models typically grounded in the concept of linear system may fail to capture the complexity of ecological processes. We have attempted to model ecosystem dynamics using a flexible, nonparametric class of nonlinear forecasting models. We analyzed annual time series of four environmental indices, 22 marine copepod taxa, and four ecologically and commercially important fish species during 1977 to 2009 on Georges Bank, a highly productive and intensively studied area of the northeast U.S. continental shelf ecosystem. We examined the underlying dynamic features of environmental indices and copepods, quantified the dynamic interactions and coherence with fishes, and explored the potential control mechanisms of ecosystem dynamics from a nonlinear perspective. We found: (1) the dynamics of marine copepods and environmental indices exhibiting clear nonlinearity; (2) little evidence of complex dynamics across taxonomic levels of copepods; (3) strong dynamic interactions and coherence between copepods and fishes; and (4) the bottom-up forcing of fishes and top-down control of copepods coexisting as target trophic levels vary. These findings highlight the nonlinear interactions among ecosystem components and the importance of marine zooplankton to fish populations which point to two forcing mechanisms likely interactively regulating the ecosystem dynamics on Georges Bank under a changing environment.","tags":["Nonlinear Dynamics","Fisheries","Time Series"],"title":"Modeling dynamic interactions and coherence between marine zooplankton and fishes linked to environmental variability","type":"publication"},{"authors":["Sarah M. Glaser","Hao Ye","George Sugihara"],"categories":null,"content":"","date":1374019200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"58fc4b470ba95a00cf70904cdbf4b1f7","permalink":"https://ha0ye.github.io/personal-website/publication/2013_spatial-forecasts/","publishdate":"2013-07-17T00:00:00Z","relpermalink":"/personal-website/publication/2013_spatial-forecasts/","section":"publication","summary":"Spatial variability can confound accurate estimates of catch per unit effort (CPUE), especially in highly migratory species. The incorporation of spatial structure into fishery stock assessment models should ultimately improve forecasts of stock biomass. Here, we describe a nonlinear time series model for producing spatially explicit forecasts of CPUE that does not require ancillary environmental or demographic data, or specification of a model functional form. We demonstrate this method using spatially resolved (1° × 1° cells) CPUE time series of North Pacific albacore in the California Current System. The spatial model is highly significant (P ","tags":["Fisheries","Forecasting","Time Series"],"title":"A nonlinear, low data requirement model for producing spatially explicit fishery forecasts","type":"publication"},{"authors":["Ethan R. Deyle","Michael Fogarty","Chih-hao Hsieh","Les Kaufman","Alec D. MacCall","Stephan B. Munch","Charles T. Perretti","Hao Ye","George Sugihara"],"categories":null,"content":"","date":1366070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"6d2f7fae9b642c4e4b6fde0cfa6042a2","permalink":"https://ha0ye.github.io/personal-website/publication/2013_sardine-climate/","publishdate":"2013-04-16T00:00:00Z","relpermalink":"/personal-website/publication/2013_sardine-climate/","section":"publication","summary":"For many marine species and habitats, climate change and overfishing present a double threat. To manage marine resources effectively, it is necessary to adapt management to changes in the physical environment. Simple relationships between environmental conditions and fish abundance have long been used in both fisheries and fishery management. In many cases, however, physical, biological, and human variables feed back on each other. For these systems, associations between variables can change as the system evolves in time. This can obscure relationships between population dynamics and environmental variability, undermining our ability to forecast changes in populations tied to physical processes. Here we present a methodology for identifying physical forcing variables based on nonlinear forecasting and show how the method provides a predictive understanding of the influence of physical forcing on Pacific sardine.","tags":["Causality","Climate","Fisheries","Time Series","Nonlinear Dynamics"],"title":"Predicting climate effects on Pacific sardine","type":"publication"},{"authors":["George Sugihara","Robert May","Hao Ye","Chih-hao Hsieh","Ethan Deyle","Michael Fogarty","Stephan Munch"],"categories":null,"content":"","date":1348099200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"fe59f5c2ba98b3901781314ffbc37ee7","permalink":"https://ha0ye.github.io/personal-website/publication/2012_causality/","publishdate":"2012-09-20T00:00:00Z","relpermalink":"/personal-website/publication/2012_causality/","section":"publication","summary":"Identifying causal networks is important for effective policy and management recommendations on climate, epidemiology, financial regulation, and much else. Here, we introduce a method, based on nonlinear state space reconstruction, that can distinguish causality from correlation. It extends to nonseparable weakly connected dynamic systems (cases not covered by the current Granger causality paradigm). The approach is illustrated both by simple models (where, in contrast to the real world, we know the underlying equations/relations and so can check the validity of our method) and by application to real ecological systems, including the controversial sardine-anchovy-temperature problem.","tags":["Causality","Time Series","Dynamics"],"title":"Detecting causality in complex ecosystems","type":"publication"},{"authors":["George Sugihara","John Beddington","Chih-hao Hsieh","Ethan Deyle","Michael Fogarty","Sarah M. Glaser","Roger Hewitt","Anne Hollowed","Robert M. May","Stephan B. Munch","Charles Perretti","Andrew A. Rosenberg","Stuart Sandin","Hao Ye"],"categories":null,"content":"","date":1322524800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"0bd503e8e06039af086795d4f6d16f60","permalink":"https://ha0ye.github.io/personal-website/publication/2011_fish-stability/","publishdate":"2011-11-29T00:00:00Z","relpermalink":"/personal-website/publication/2011_fish-stability/","section":"publication","summary":"","tags":["Fisheries","Time Series","Stability","Nonlinear Dynamics","Dynamics"],"title":"Are exploited fish populations stable?","type":"publication"},{"authors":["Sarah M. Glaser","Hao Ye","Mark Maunder","Alec MacCall","Michael Fogarty","George Sugiharaa"],"categories":null,"content":"","date":1297296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"b57be799cd20c460ffd69b9871e10286","permalink":"https://ha0ye.github.io/personal-website/publication/2011_spatial-forecasts/","publishdate":"2011-02-10T00:00:00Z","relpermalink":"/personal-website/publication/2011_spatial-forecasts/","section":"publication","summary":"The presence of complex, nonlinear dynamics in fish populations, and uncertainty in the structure (functional form) of those dynamics, pose challenges to the accuracy of forecasts produced by traditional stock assessment models. We describe two nonlinear forecasting models that test for the hallmarks of complex behavior, avoid problems of structural uncertainty, and produce good forecasts of catch-per-unit-effort (CPUE) time series in both standardized and nominal (unprocessed) form. We analyze a spatially extensive, 40-year-long data set of annual CPUE time series of North Pacific albacore (Thunnus alalunga) from 1° × 1° cells from the eastern North Pacific Ocean. The use of spatially structured data in compositing techniques improves out-of-sample forecasts of CPUE and overcomes difficulties commonly encountered when using short, incomplete time series. These CPUE series display low-dimensional, nonlinear structure and significant predictability. Such characteristics have important implications for industry efficiency in terms of future planning and can inform formal stock assessments used for the management of fisheries.","tags":["Fisheries","Forecasting","Time Series"],"title":"Detecting and forecasting complex nonlinear dynamics in spatially structured catch-per-unit-effort time series for North Pacific albacore (Thunnus alalunga)","type":"publication"},{"authors":["George Sugihara","Hao Ye"],"categories":null,"content":"","date":1240444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"138d0638a129b94de5f1a13ace5818c8","permalink":"https://ha0ye.github.io/personal-website/publication/2009_cooperative-dynamics/","publishdate":"2009-04-23T00:00:00Z","relpermalink":"/personal-website/publication/2009_cooperative-dynamics/","section":"publication","summary":"Nested, or hierarchically arranged, mutualisms allow ecosystems to support more species than they otherwise would. But in this and other contexts, the growth of such networks could carry a heavy price.","tags":["Complexity","Networks","Dynamics"],"title":"Complex systems: Cooperative network dynamics","type":"publication"},{"authors":["A. Kilcik","C. N. K. Anderson","J. P. Rozelot","H. Ye","G. Sugihara","A. Ozguc"],"categories":null,"content":"","date":1236211200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"2f23f97f9e7bccea2c940c57595bf1aa","permalink":"https://ha0ye.github.io/personal-website/publication/2009_solar-cycle-prediction/","publishdate":"2009-03-05T00:00:00Z","relpermalink":"/personal-website/publication/2009_solar-cycle-prediction/","section":"publication","summary":"Sunspot activity is highly variable and challenging to forecast. Yet forecasts are important, since peak activity has profound effects on major geophysical phenomena including space weather (satellite drag, telecommunications outages) and has even been correlated speculatively with changes in global weather patterns. This paper investigates trends in sunspot activity, using new techniques for decadal-scale prediction of the present solar cycle (cycle 24). First, Hurst exponent H analysis is used to investigate the autocorrelation structure of the putative dynamics; then the Sugihara-May algorithm is used to predict the ascension time and the maximum intensity of the current sunspot cycle. Here we report H = 0.86 for the complete sunspot number data set (1700-2007) and H = 0.88 for the reliable sunspot data set (1848-2007). Using the Sugihara-May algorithm analysis, we forecast that cycle 24 will reach its maximum in 2012 December at approximately 87 sunspot units.","tags":["Forecasting","Sunspots","Time Series"],"title":"Nonlinear prediction of solar cycle 24","type":"publication"},{"authors":["Mark A. Changizi","Qiong Zhang","Hao Ye","Shinsuke Shimojo"],"categories":null,"content":"","date":1143072000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673207528,"objectID":"208adcd95e84d3db99e70e5364f5a84b","permalink":"https://ha0ye.github.io/personal-website/publication/2006_language-symbols/","publishdate":"2006-03-23T00:00:00Z","relpermalink":"/personal-website/publication/2006_language-symbols/","section":"publication","summary":"Are there empirical regularities in the shapes of letters and other human visual signs, and if so, what are the selection pressures underlying these regularities? To examine this, we determined a wide variety of topologically distinct contour configurations and examined the relative frequency of these configuration types across writing systems, Chinese writing, and nonlinguistic symbols. Our first result is that these three classes of human visual sign possess a similar signature in their configuration distribution, suggesting that there are underlying principles governing the shapes of human visual signs. Second, we provide evidence that the shapes of visual signs are selected to be easily seen at the expense of the motor system. Finally, we provide evidence to support an ecological hypothesis that visual signs have been culturally selected to match the kinds of conglomeration of contours found in natural scenes because that is what we have evolved to be good at visually processing.","tags":["Complexity","Human Vision"],"title":"The structures of letters and symbols throughout human history are selected to match those found in objects in natural scenes","type":"publication"}]